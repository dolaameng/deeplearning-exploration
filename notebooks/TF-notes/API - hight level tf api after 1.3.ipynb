{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# tensorflow highlevel api after 1.3 part 1 summary\n",
    "\n",
    "## resources\n",
    "- [higher-level apis in tensorflow](https://medium.com/onfido-tech/higher-level-apis-in-tensorflow-67bfb602e6c0)\n",
    "- [example of tensorflows new input pipeline](https://kratzert.github.io/2017/06/15/example-of-tensorflows-new-input-pipeline.html)\n",
    "- [introduction to tensorflow datasets and estimators](https://developers.googleblog.com/2017/09/introducing-tensorflow-datasets.html)\n",
    "- [cloudml census example](https://github.com/GoogleCloudPlatform/cloudml-samples/tree/master/census/customestimator/trainer)\n",
    "- [using tensorflow api for structured data - another census example](https://github.com/tensorflow/workshops/blob/master/notebooks/07_structured_data.ipynb)\n",
    "- [Example of TensorFlows new Input Pipeline](https://kratzert.github.io/2017/06/15/example-of-tensorflows-new-input-pipeline.html)\n",
    "- [programmer's guide - dataset](https://www.tensorflow.org/programmers_guide/datasets)\n",
    "- [programmer's guide - estimators](https://www.tensorflow.org/programmers_guide/estimators)\n",
    "- [api docs: feature_column](https://www.tensorflow.org/api_docs/python/tf/contrib/layers/feature_column)\n",
    "- [wide and deep learning tutorial](https://www.tensorflow.org/tutorials/wide_and_deep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1 Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### high level components and structure  - from [higher-level apis in tensorflow](https://medium.com/onfido-tech/higher-level-apis-in-tensorflow-67bfb602e6c0)\n",
    "\n",
    "<img src=\"https://cdn-images-1.medium.com/max/1600/1*zoNZvvuJb06yAghetc6BfQ.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2 dataset\n",
    "\n",
    "### dataset construction\n",
    "- from source, e.g., memory (placeholder, constant tensor and etc), hard disk (tfrecord, images, text files)\n",
    "- from transformation\n",
    "\n",
    "### main interface\n",
    "- iterator (one-shot, initializable, reinitailizable, feedable)\n",
    "\n",
    "### estimator needs a input_fn that returns a two-element tuptle organized as follows:\n",
    "- The first element must be a dict in which each input feature is a key, and then a list of values for the training batch.\n",
    "- The second element is a list of labels for the training batch.\n",
    "For example,\n",
    "\n",
    "\n",
    "```python\n",
    "def input_fn():\n",
    "    ...<code>...\n",
    "    return ({ 'SepalLength':[values], ..<etc>.., 'PetalWidth':[values] },\n",
    "            [IrisFlowerType])\n",
    "```\n",
    "\n",
    "### useful packages\n",
    "- tf decoder, e.g., `tf.decode_csv`, parsing data into tensors\n",
    "- tf dataset from source, e.g., `tf.TextLineDataset`\n",
    "- main transformation:\n",
    "    - `map`\n",
    "    - `shuffle`\n",
    "    - `batch`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.3.0'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "tf.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Dataset CSV Example\n",
    "- use `data.TextLineDataset` interface\n",
    "- iris data\n",
    "    - train: http://download.tensorflow.org/data/iris_training.csv\n",
    "    - test: http://download.tensorflow.org/data/iris_test.csv\n",
    "- sometime you might want to just use the pandas input function:\n",
    "    - `estimator.inputs.pandas_input_fn`\n",
    "    - but using a dataset api has a nicer interface and scales better"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from tensorflow.contrib import data # might move to core later \n",
    "\n",
    "# define the input function for estimator/experiment - factory method\n",
    "# use tf Dataset api, with text line data source\n",
    "\n",
    "\n",
    "def iris_input_fn(file_path, perform_shuffle=False, repeat_count=1):\n",
    "    \"\"\"input_fn returns access iterator to a dataset\n",
    "    \"\"\"\n",
    "    feature_names = [\"sepallength\", \"sepalwidth\", \"petallength\", \"petalwidth\"]\n",
    "    def decode_csv(line):\n",
    "        # parsed as tensors\n",
    "        # default values also define the dtype\n",
    "        parsed_line = tf.decode_csv(line, [[0.], [0.], [0.], [0.], [0]])\n",
    "        label = parsed_line[-1:] # list of the last element as label\n",
    "        features = parsed_line[:-1]\n",
    "        # return tuple of (feature_dict, label_list)\n",
    "        return (dict(zip(feature_names, features)), label)\n",
    "    dataset = (data.TextLineDataset(file_path)\n",
    "                   .skip(1) # skip header\n",
    "                   .map(decode_csv, \n",
    "                        num_threads=4, # running in parallel\n",
    "                        output_buffer_size=100*32)) # transform each line to a tuple\n",
    "    if perform_shuffle:\n",
    "        # randomize input using a window of 256 in memory, should be bigger than batch?\n",
    "        dataset.shuffle(buffer_size=256)\n",
    "    dataset = dataset.repeat(repeat_count) # repeat dataset # times\n",
    "    dataset = dataset.batch(32) # batch to use\n",
    "    # main interface as an iterator\n",
    "    iterator = dataset.make_one_shot_iterator()\n",
    "    batch_features, batch_labels = iterator.get_next()\n",
    "    return batch_features, batch_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'petallength': <tf.Tensor 'IteratorGetNext_2:0' shape=(?,) dtype=float32>,\n",
       "  'petalwidth': <tf.Tensor 'IteratorGetNext_2:1' shape=(?,) dtype=float32>,\n",
       "  'sepallength': <tf.Tensor 'IteratorGetNext_2:2' shape=(?,) dtype=float32>,\n",
       "  'sepalwidth': <tf.Tensor 'IteratorGetNext_2:3' shape=(?,) dtype=float32>},\n",
       " <tf.Tensor 'IteratorGetNext_2:4' shape=(?, 1) dtype=int32>)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_batch, y_batch = iris_input_fn(\"../../data/iris/iris_training.csv\")\n",
    "x_batch, y_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32, 1)\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    print(sess.run(y_batch).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Dataset from Memory\n",
    "- for small dataset that can be fit into the memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# array in memories\n",
    "prediction_input = [[5.9, 3.0, 4.2, 1.5],  # -> 1, Iris Versicolor\n",
    "                    [6.9, 3.1, 5.4, 2.1],  # -> 2, Iris Virginica\n",
    "                    [5.1, 3.3, 1.7, 0.5]]  # -> 0, Iris Sentosa\n",
    "\n",
    "def memory_input_fn():\n",
    "    # data meta information should probably be defined globally after so many redundance\n",
    "    feature_names = [\"sepallength\", \"sepalwidth\", \"petallength\", \"petalwidth\"]\n",
    "    def decode(row):\n",
    "        # always use the tf version, because it spits out tensors\n",
    "        feats = tf.split(row, 4)\n",
    "        # don't need the label when predicting\n",
    "        return dict(zip(feature_names, feats))\n",
    "    # use a memory structure as input\n",
    "    dataset = (data.Dataset.from_tensor_slices(prediction_input)\n",
    "                           .map(decode))\n",
    "    iterator = dataset.make_one_shot_iterator()\n",
    "    batch_x = iterator.get_next()\n",
    "    # you still need the tuple for data pipeline\n",
    "    return batch_x, None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3 estimator\n",
    "- the most important interface to an estimator is the data pipe, defined as `input_fn`\n",
    "- `input_fn`\n",
    "    - takes no parameters\n",
    "    - always return `(batch_x, batch_y)`, where both are tensors\n",
    "    - usually it returns the result of `data_iterator.get_next()`, where `data_iterator` is the iterator interface to a dataset\n",
    "- for an estimator to use a data pipe, you need to provide the \"adaptor\" between them, which is defined as `feature_columns` parameter. It is essentially an adaptor from original features to transformed features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from tensorflow import estimator\n",
    "from functools import partial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy : 0.9333333373069763\n",
      "average_loss : 0.3552239239215851\n",
      "loss : 10.656717300415039\n",
      "global_step : 30\n"
     ]
    }
   ],
   "source": [
    "# estimiator definition starts with meta-data definition, e.g. columns\n",
    "feature_names = [\"sepallength\", \"sepalwidth\", \"petallength\", \"petalwidth\"]\n",
    "feature_columns = [tf.feature_column.numeric_column(f) for f in feature_names]\n",
    "\n",
    "# use a predefined model\n",
    "# subsequent run will try to load the model from model_dir,\n",
    "# remove it if you want a fresh run\n",
    "classifier = estimator.DNNClassifier(\n",
    "    hidden_units=[10, 10], \n",
    "    feature_columns=feature_columns, \n",
    "    model_dir=\"../../models/iris/\", \n",
    "    n_classes=3)\n",
    "\n",
    "# train the model on the input pipe\n",
    "# it takes a function as parameter, because it may need to recreate \n",
    "# the tensors sometime, estimators need input_fn with no argument\n",
    "train_path = \"../../data/iris/iris_training.csv\"\n",
    "classifier.train(input_fn=partial(iris_input_fn, file_path=train_path, perform_shuffle=True, repeat_count=8))\n",
    "\n",
    "# evaluate on validation set\n",
    "test_path = \"../../data/iris/iris_test.csv\"\n",
    "eval_results = classifier.evaluate(input_fn=lambda : iris_input_fn(test_path, perform_shuffle=False, repeat_count=1))\n",
    "for key in eval_results:\n",
    "    print(\"{} : {}\".format(key, eval_results[key]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Input graph does not contain a QueueRunner. That means predict yields forever. This is probably a mistake.\n",
      "{'logits': array([-0.17707157,  2.25011516,  1.21196556], dtype=float32), 'probabilities': array([ 0.06120716,  0.69329178,  0.24550109], dtype=float32), 'class_ids': array([1]), 'classes': array([b'1'], dtype=object)}\n"
     ]
    }
   ],
   "source": [
    "## make predictions, reading from files on disk\n",
    "pred_results = classifier.predict(input_fn=lambda: iris_input_fn(test_path, perform_shuffle=False, repeat_count=1))\n",
    "for pred in pred_results:\n",
    "    print(pred)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Input graph does not contain a QueueRunner. That means predict yields forever. This is probably a mistake.\n",
      "{'logits': array([-0.17707151,  2.25011492,  1.21196592], dtype=float32), 'probabilities': array([ 0.06120717,  0.69329166,  0.24550118], dtype=float32), 'class_ids': array([1]), 'classes': array([b'1'], dtype=object)}\n",
      "{'logits': array([-1.29501081,  3.1084075 ,  3.03103995], dtype=float32), 'probabilities': array([ 0.00631414,  0.51605314,  0.47763279], dtype=float32), 'class_ids': array([1]), 'classes': array([b'1'], dtype=object)}\n",
      "{'logits': array([ 3.25232768,  0.65270334, -2.50874376], dtype=float32), 'probabilities': array([ 0.92811799,  0.06896054,  0.00292147], dtype=float32), 'class_ids': array([0]), 'classes': array([b'0'], dtype=object)}\n"
     ]
    }
   ],
   "source": [
    "## prediction with data pipe on memory values\n",
    "pred_results = classifier.predict(input_fn=memory_input_fn)\n",
    "for pred_result in pred_results:\n",
    "    print(pred_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4 experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5 some utility function\n",
    "- `tf.app` - argumentparser, main function\n",
    "- `tf.gfile` - shutil, os, glob and etc\n",
    "- `tf.logging`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### usage of `tf.app`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./models'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## set and get and command line parameters\n",
    "FLAGS = tf.app.flags.FLAGS\n",
    "tf.app.flags.DEFINE_string(flag_name='saved_model_dir',\n",
    "                           default_value='./models',\n",
    "                           docstring='Output dir for model and training sets')\n",
    "# later access it by\n",
    "FLAGS.saved_model_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## run the main function\n",
    "def main_fn(argv=None):\n",
    "    pass\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    tf.app.run(main=main_fn, argv=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### usage of `tf.gfile`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function tensorflow.python.lib.io.file_io.delete_recursively>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.gfile.Exists\n",
    "tf.gfile.IsDirectory\n",
    "tf.gfile.Glob\n",
    "tf.gfile.GFile\n",
    "tf.gfile.DeleteRecursively"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### usage of tf.logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:on logging\n"
     ]
    }
   ],
   "source": [
    "tf.logging.set_verbosity(tf.logging.WARN)\n",
    "tf.logging.warn(\"on logging\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 1: adult census data\n",
    "- pipeline for feature transformation using census data\n",
    "- tensorflow for structured data\n",
    "- the values in the csv always starts with an extra whitespace\n",
    "- get the data\n",
    "    - train: https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.data\n",
    "    - test: https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.test\n",
    "    \n",
    "    \n",
    "### Notes\n",
    "- to build solution you need `data` and `model`\n",
    "- `data` is defined as a pipe\n",
    "    - in the format of `input_fn()`, which will return a tuple of batch every time when it is called\n",
    "    - it is usually implemented by data api, with the call of iterator.get_next()\n",
    "- `model` is defined in\n",
    "    - `model_fn` that returns `EstimatorSpec`, defining computation graph and how to run TRAIN/EVAL/PREDICT\n",
    "    - `input_fn` that is used for train/eval/predict, to provide original data\n",
    "    - `feature_columns` that glue/transform model with data pipe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### step 1 use pandas to explore the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>workclass</th>\n",
       "      <th>fnlwgt</th>\n",
       "      <th>education</th>\n",
       "      <th>education-num</th>\n",
       "      <th>marital-status</th>\n",
       "      <th>occupation</th>\n",
       "      <th>relationship</th>\n",
       "      <th>race</th>\n",
       "      <th>gender</th>\n",
       "      <th>capital-gain</th>\n",
       "      <th>capital-loss</th>\n",
       "      <th>hours-per-week</th>\n",
       "      <th>native-country</th>\n",
       "      <th>income</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>39</td>\n",
       "      <td>State-gov</td>\n",
       "      <td>77516</td>\n",
       "      <td>Bachelors</td>\n",
       "      <td>13</td>\n",
       "      <td>Never-married</td>\n",
       "      <td>Adm-clerical</td>\n",
       "      <td>Not-in-family</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>2174</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>50</td>\n",
       "      <td>Self-emp-not-inc</td>\n",
       "      <td>83311</td>\n",
       "      <td>Bachelors</td>\n",
       "      <td>13</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Exec-managerial</td>\n",
       "      <td>Husband</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>38</td>\n",
       "      <td>Private</td>\n",
       "      <td>215646</td>\n",
       "      <td>HS-grad</td>\n",
       "      <td>9</td>\n",
       "      <td>Divorced</td>\n",
       "      <td>Handlers-cleaners</td>\n",
       "      <td>Not-in-family</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>53</td>\n",
       "      <td>Private</td>\n",
       "      <td>234721</td>\n",
       "      <td>11th</td>\n",
       "      <td>7</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Handlers-cleaners</td>\n",
       "      <td>Husband</td>\n",
       "      <td>Black</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>28</td>\n",
       "      <td>Private</td>\n",
       "      <td>338409</td>\n",
       "      <td>Bachelors</td>\n",
       "      <td>13</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Prof-specialty</td>\n",
       "      <td>Wife</td>\n",
       "      <td>Black</td>\n",
       "      <td>Female</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>Cuba</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   age          workclass  fnlwgt   education  education-num  \\\n",
       "0   39          State-gov   77516   Bachelors             13   \n",
       "1   50   Self-emp-not-inc   83311   Bachelors             13   \n",
       "2   38            Private  215646     HS-grad              9   \n",
       "3   53            Private  234721        11th              7   \n",
       "4   28            Private  338409   Bachelors             13   \n",
       "\n",
       "        marital-status          occupation    relationship    race   gender  \\\n",
       "0        Never-married        Adm-clerical   Not-in-family   White     Male   \n",
       "1   Married-civ-spouse     Exec-managerial         Husband   White     Male   \n",
       "2             Divorced   Handlers-cleaners   Not-in-family   White     Male   \n",
       "3   Married-civ-spouse   Handlers-cleaners         Husband   Black     Male   \n",
       "4   Married-civ-spouse      Prof-specialty            Wife   Black   Female   \n",
       "\n",
       "   capital-gain  capital-loss  hours-per-week  native-country  income  \n",
       "0          2174             0              40   United-States   <=50K  \n",
       "1             0             0              13   United-States   <=50K  \n",
       "2             0             0              40   United-States   <=50K  \n",
       "3             0             0              40   United-States   <=50K  \n",
       "4             0             0              40            Cuba   <=50K  "
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "column_names = [\n",
    "  'age', 'workclass', 'fnlwgt', 'education', 'education-num',\n",
    "  'marital-status', 'occupation', 'relationship', 'race', 'gender',\n",
    "  'capital-gain', 'capital-loss', 'hours-per-week', 'native-country',\n",
    "  'income'\n",
    "]\n",
    "\n",
    "df = pd.read_csv(\"../../data/adult/adult.data\", names=column_names)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### step 2 build data pipe with dataset api\n",
    "- the output is the `input_fn` for an estimator\n",
    "- use dataset api might have more pros compared to directly reading from pandas\n",
    "- common practice:\n",
    "    - build the pipe with meta data (e.g., original feature names, default values), transformed feature columns with feature_column api\n",
    "    - use data api to read from data source and pipe it with transformed columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from tensorflow import estimator, feature_column\n",
    "from tensorflow.contrib import data\n",
    "from collections import OrderedDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# it is usually good practice to start with original data name and default values (and thus dtype), \n",
    "# as the schema of the data - order is important\n",
    "\n",
    "csv_column_defaults = OrderedDict([\n",
    "    ('age', [0]),\n",
    "    ('workclass', ['']),\n",
    "    ('fnlwgt', [0]),\n",
    "    ('education', ['']),\n",
    "    ('education-num', [0]),\n",
    "    ('marital-status', ['']),\n",
    "    ('occupation', ['']),\n",
    "    ('relationship', ['']),\n",
    "    ('race', ['']),\n",
    "    ('gender', ['']),\n",
    "    ('capital-gain', [0]),\n",
    "    ('capital-loss', [0]),\n",
    "    ('hours-per-week', [0]),\n",
    "    ('native-country', ['']),\n",
    "    ('income', [''])\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['age', 'capital-gain', 'capital-loss', 'education', 'education-num', 'fnlwgt', 'gender', 'hours-per-week', 'marital-status', 'native-country', 'occupation', 'race', 'relationship', 'workclass']) dict_keys(['age', 'capital-gain', 'capital-loss', 'education', 'education-num', 'fnlwgt', 'gender', 'hours-per-week', 'marital-status', 'native-country', 'occupation', 'race', 'relationship', 'workclass'])\n",
      "(32,) (32,)\n"
     ]
    }
   ],
   "source": [
    "## then we need to define the parser from the data source, i.e., csv in this case\n",
    "## using dataset api\n",
    "\n",
    "def csv_decoder(line):\n",
    "    parsed = tf.decode_csv(line, list(csv_column_defaults.values())) # to tensors\n",
    "    # return dict\n",
    "    return dict(zip(csv_column_defaults.keys(), parsed))\n",
    "\n",
    "# helper function\n",
    "def filter_empty_lines(line):\n",
    "    # always use tf operator, return true is line is empty\n",
    "    return tf.not_equal(0, tf.size(tf.string_split([line], ', ').values))\n",
    "\n",
    "# define train and test pipe (aka input_fn)\n",
    "# every time the input_fn is called, it spits out a batch\n",
    "def get_input_fn(path, ignore_lines=0, shuffle=False, repeats=1):\n",
    "    def input_fn():\n",
    "        # create dataset\n",
    "        dataset = (data.TextLineDataset(path)\n",
    "                   .skip(ignore_lines) # the first line in test file should be passed\n",
    "                   .filter(filter_empty_lines)\n",
    "                   .map(csv_decoder))\n",
    "        if shuffle:\n",
    "            dataset = dataset.shuffle(buffer_size=200)\n",
    "        dataset = dataset.repeat(repeats).batch(32)\n",
    "        # create iterator and batch variables\n",
    "        feats = dataset.make_one_shot_iterator().get_next()\n",
    "        income = tf.equal(feats.pop('income'), \" >50K\")\n",
    "        return (feats, income)\n",
    "    return input_fn\n",
    "\n",
    "## test the pipe\n",
    "with tf.Session() as sess:\n",
    "    train_input_fn = get_input_fn(\"../../data/adult/adult.data\", shuffle=True, repeats=5)\n",
    "    test_input_fn = get_input_fn(\"../../data/adult/adult.test\", ignore_lines=1, shuffle=False, repeats=1)\n",
    "    train_feats, train_income = sess.run(train_input_fn())\n",
    "    test_feats, test_income = sess.run(test_input_fn())\n",
    "    print(train_feats.keys(), test_feats.keys())\n",
    "    print(train_income.shape, test_income.shape)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# what tf.string_split does\n",
    "with tf.Session() as sess:\n",
    "    a = tf.string_split(['hello, world', \"ab, c\"], ', ').values # sparse array so .values\n",
    "    print(sess.run(a))\n",
    "    \n",
    "# output >>> [b'hello' b'world' b'ab' b'c']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### setp 3 define adaptor between data pipe and estimiator using feature_column api\n",
    "- common feature represenations by feature_column api\n",
    "    - A numeric_column. This is just a real-valued attribute.\n",
    "    - A bucketized_column. TensorFlow automatically buckets a numeric column for us.\n",
    "    - A categorical_column_with_vocabulary_list. This is just a categorical column, where you know the possible values in advance. This is useful when you have a small number of possibilities.\n",
    "    - A categorical_column_with_hash_bucket. This is a useful way to represent categorical features when you have a large number of values. Beware of hash collisions.\n",
    "    - A crossed_column. Linear models cannot consider interactions between features, so we'll ask TensorFlow to cross features for us.\n",
    "- common feature transformations:\n",
    "    - feature_column.indicator_column\n",
    "    - feature_column.embedding_column\n",
    "- use `tf.feature_column.input_layer` to pipe the data into engineered features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hour_per_week = feature_column.numeric_column('hours-per-week')\n",
    "education_num = feature_column.bucketized_column( # represented features\n",
    "                    feature_column.numeric_column('education-num'), # raw feature\n",
    "                    list(range(10)))\n",
    "age_buckets = feature_column.bucketized_column(\n",
    "                    feature_column.numeric_column('age'), \n",
    "                    boundaries=[18, 25, 30, 35, 40, 45, 50, 55, 60, 65])\n",
    "sex = feature_column.categorical_column_with_vocabulary_list('gender', ['male', 'female'])\n",
    "native_country = feature_column.categorical_column_with_hash_bucket('native-country', 1000)\n",
    "## a crossed column\n",
    "education_num_x_gender = feature_column.crossed_column([education_num, sex], hash_bucket_size=int(1e4))\n",
    "\n",
    "estimator_columns = [\n",
    "    hour_per_week,\n",
    "    education_num,\n",
    "    age_buckets,\n",
    "    feature_column.indicator_column(sex),\n",
    "    feature_column.embedding_column(native_country, dimension=10),\n",
    "    feature_column.embedding_column(education_num_x_gender, 20)\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### step 4 test it with a predefined model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model_dir = \"../../models/adult\"\n",
    "if tf.gfile.Exists(model_dir):\n",
    "    tf.gfile.DeleteRecursively(model_dir)\n",
    "classifier = estimator.DNNClassifier([64, 64], estimator_columns, \n",
    "                                     model_dir=model_dir, n_classes=2, optimizer=\"Adam\")\n",
    "train_input_fn = get_input_fn(\"../../data/adult/adult.data\", shuffle=True, repeats=8)\n",
    "test_input_fn = get_input_fn(\"../../data/adult/adult.test\", ignore_lines=1, shuffle=False, repeats=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 26s, sys: 35.2 s, total: 2min 1s\n",
      "Wall time: 59 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.estimator.canned.dnn.DNNClassifier at 0x11f11fef0>"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%time classifier.train(input_fn=train_input_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Casting <dtype: 'float32'> labels to bool.\n",
      "WARNING:tensorflow:Casting <dtype: 'float32'> labels to bool.\n",
      "CPU times: user 8.19 s, sys: 2.61 s, total: 10.8 s\n",
      "Wall time: 7.14 s\n"
     ]
    }
   ],
   "source": [
    "%time eval_result = classifier.evaluate(test_input_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 1.0\n",
      "accuracy_baseline: 1.0\n",
      "auc: 1.0\n",
      "auc_precision_recall: 0.0\n",
      "average_loss: 0.2827436327934265\n",
      "label/mean: 0.0\n",
      "loss: 9.04390811920166\n",
      "prediction/mean: 0.2462848722934723\n",
      "global_step: 8141\n"
     ]
    }
   ],
   "source": [
    "for k, v in eval_result.items():\n",
    "    print(\"{}: {}\".format(k, v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
