{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using auxiliary task to train MNIST classification\n",
    "Details can be found in Chapter 11 of Aurélien Géron book \"Hands-On Machine Learning with Scikit-Learn and TensorFlow\"\n",
    "\n",
    "- auxiliary task: train two dnn models and combine the output to tell whether two images are from the same digits.\n",
    "- after that, reuse the one of the dnn model, extend it to train a mnist classification model on a much smaller dataset (5k images, around 500 for each digit)\n",
    "- it shows my exploration of how to save/restore/reuse models in tensorflow(0.12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_mldata\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.contrib.framework import arg_scope\n",
    "from tensorflow.contrib.layers import fully_connected, batch_norm\n",
    "from tensorflow.contrib.layers import variance_scaling_initializer\n",
    "from tensorflow.contrib.layers import dropout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load mnist, split into 2 parts\n",
    "- #1 for auxiliary training, same or not\n",
    "- #2 5000 images, traditional mnist classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_mnist():\n",
    "    mnist = fetch_mldata(\"MNIST Original\")\n",
    "    # without normalizing input, bcs we expect to use batch normalization\n",
    "    X, y = mnist.data.astype(np.float32), mnist.target.astype(np.int32)\n",
    "#     X /= 255 # not needed if batch norm is used\n",
    "    X, y = shuffle(X, y)\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_batch_generator(X, y, batch_size=64):\n",
    "    i, n = 0, X.shape[0]\n",
    "    while True:\n",
    "        i %= n\n",
    "        yield X[i:i+batch_size], y[i:i+batch_size]\n",
    "        i += batch_size\n",
    "        if i >= n: i = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X, y = load_mnist()\n",
    "X1, X2, y1, y2 = train_test_split(X, y, test_size=5000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### train for identical image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1000, 784), (1000, 784), (1000,))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# slow but good enough for a demo\n",
    "def pair_batch_generator(X, y, batch_size=64):\n",
    "    idx = np.arange(X.shape[0])\n",
    "    while True:\n",
    "        selected = np.random.choice(idx, batch_size)\n",
    "        x1 = X[selected]\n",
    "        x2 = []\n",
    "        for i, label in enumerate(y[selected]):\n",
    "            if i % 2 == 0:\n",
    "                choice = np.random.choice(np.where(y==label)[0], 1)\n",
    "            else:\n",
    "                choice = np.random.choice(np.where(y!=label)[0], 1)\n",
    "            x2.append(X[choice])\n",
    "        x2 = np.concatenate(x2)\n",
    "        yy = np.tile([0, 1], batch_size//2).astype(np.int32)\n",
    "        yield x1, x2, yy\n",
    "        \n",
    "pair_batches = pair_batch_generator(X1, y1)\n",
    "test_x1, test_x2, test_yy = next(pair_batch_generator(X1, y1, 1000))\n",
    "test_x1.shape, test_x2.shape, test_yy.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "n_inputs = 28 * 28\n",
    "n_hiddens = [100] * 5\n",
    "batch_size = 64\n",
    "n_epoches = 20\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "x1 = tf.placeholder(tf.float32, [None, n_inputs], name=\"x1\")\n",
    "x2 = tf.placeholder(tf.float32, [None, n_inputs], name=\"x2\")\n",
    "yy = tf.placeholder(tf.int32, [None], name=\"yy\")\n",
    "is_training = tf.placeholder(tf.bool, [], name=\"is_training\")\n",
    "keep_prob = 0.5\n",
    "\n",
    "bn_params = {\n",
    "    \"is_training\": is_training,\n",
    "    \"decay\": 0.9,\n",
    "    \"updates_collections\": None,\n",
    "    \"scale\": True\n",
    "}\n",
    "he_init = variance_scaling_initializer()\n",
    "\n",
    "with tf.name_scope(\"dnn\"):\n",
    "    with arg_scope([fully_connected],\n",
    "                  activation_fn=tf.nn.elu,\n",
    "                  normalizer_fn=batch_norm,\n",
    "                  normalizer_params=bn_params,\n",
    "                  weights_initializer=he_init):\n",
    "        prev1, prev2 = x1, x2\n",
    "        for i, n_hidden in enumerate(n_hiddens):\n",
    "            h1 = fully_connected(prev1, n_hidden,\n",
    "                                 scope=\"dnn1/hidden%i\"%i)\n",
    "            h1 = dropout(h1, keep_prob, is_training=is_training)\n",
    "            h2 = fully_connected(prev2, n_hidden,\n",
    "                                 scope=\"dnn2/hidden%i\"%i)\n",
    "            h2 = dropout(h2, keep_prob, is_training=is_training)\n",
    "            prev1, prev2 = h1, h2\n",
    "        h = tf.concat(1, [h1, h2])\n",
    "        hh = fully_connected(h, 100, scope=\"output-2\")\n",
    "        hh = dropout(hh, keep_prob, is_training=is_training)\n",
    "        logits = fully_connected(hh, 2, activation_fn=None, scope=\"output\")\n",
    "#         logits = tf.squeeze(logits, axis=1)\n",
    "        \n",
    "with tf.name_scope(\"loss\"):\n",
    "#     cast_yy = tf.cast(yy, tf.float32)\n",
    "#     xentropy = -cast_yy * tf.log(logits) - (1-cast_yy) * tf.log(1-logits)\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(logits, yy)\n",
    "    loss = tf.reduce_mean(xentropy)\n",
    "    \n",
    "with tf.name_scope(\"train\"):\n",
    "    optimizer = tf.train.AdamOptimizer()\n",
    "    train_op = optimizer.minimize(loss)\n",
    "    \n",
    "with tf.name_scope(\"eval\"):\n",
    "#     labels = tf.cast(logits >= 0.5, tf.int32)\n",
    "#     match = tf.equal(labels, yy)\n",
    "    match = tf.nn.in_top_k(logits, yy, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(match, tf.float32))\n",
    "    \n",
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "#     saver.restore(sess, \"./same_digit_model.ckpt\")\n",
    "    for i in range(n_epoches * 55000 // batch_size):\n",
    "        b_x1, b_x2, b_yy = next(pair_batches)\n",
    "        sess.run(train_op, feed_dict={x1: b_x1, x2: b_x2, yy: b_yy, is_training:True})\n",
    "        if i % 1000 == 0:\n",
    "            train_loss, train_acc = sess.run([loss, accuracy], \n",
    "                        feed_dict={x1: b_x1, x2: b_x2, yy: b_yy, is_training:False})\n",
    "            test_acc = sess.run(accuracy,\n",
    "                        feed_dict={x1: test_x1, x2: test_x2, yy: test_yy, is_training:False})\n",
    "            print(train_loss, train_acc, test_acc)\n",
    "    save_path = saver.save(sess, \"same_digit_model.ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-rw-r--r-- 1 dola dola 3.0M Feb 16 08:13 same_digit_model.ckpt.data-00000-of-00001\r\n",
      "-rw-r--r-- 1 dola dola 4.8K Feb 16 08:13 same_digit_model.ckpt.index\r\n",
      "-rw-r--r-- 1 dola dola 1.3M Feb 16 08:13 same_digit_model.ckpt.meta\r\n"
     ]
    }
   ],
   "source": [
    "!ls -lh same_digit_model.ckpt*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### reconstruct dnn1 for mnist classification on dataset 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((4250, 784), (750, 784), (4250,), (750,))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X2, y2, test_size=0.15)\n",
    "X_train.shape, X_test.shape, y_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "n_epoches = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_batches = make_batch_generator(X_train, y_train, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# load the meta data into default graph\n",
    "tf.reset_default_graph()\n",
    "saver = tf.train.import_meta_graph(\"./same_digit_model.ckpt.meta\")\n",
    "default_graph = tf.get_default_graph()\n",
    "\n",
    "# reuse dnn1 and extend it\n",
    "n_outputs = 10\n",
    "# so now you can read the existing variables from saved graph\n",
    "X = default_graph.get_tensor_by_name(\"x1:0\")\n",
    "h = default_graph.get_tensor_by_name(\"dnn/dnn1/hidden4/Elu:0\")\n",
    "is_training = default_graph.get_tensor_by_name(\"is_training:0\")\n",
    "y = tf.placeholder(tf.int32, [None], \"y\")\n",
    "\n",
    "he_init = variance_scaling_initializer()\n",
    "bn_params = {\n",
    "    \"is_training\": is_training,\n",
    "    \"decay\": 0.9,\n",
    "    \"scale\": True,\n",
    "    \"updates_collections\": None\n",
    "}\n",
    "hh = fully_connected(X, 100, \n",
    "                         activation_fn=tf.nn.elu,\n",
    "                         normalizer_fn=batch_norm,\n",
    "                         normalizer_params=bn_params,\n",
    "                         weights_initializer=he_init, \n",
    "                         scope=\"mnist_hidden\")\n",
    "hh = dropout(hh, keep_prob=0.5, is_training=is_training, scope=\"mnist_dropout\")\n",
    "logits = fully_connected(hh, n_outputs, \n",
    "                         activation_fn=None,\n",
    "                         normalizer_fn=batch_norm,\n",
    "                         normalizer_params=bn_params,\n",
    "                         weights_initializer=he_init, \n",
    "                         scope=\"mnist_output\")\n",
    "\n",
    "xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(logits, y)\n",
    "loss = tf.reduce_mean(xentropy)\n",
    "\n",
    "optimizer = tf.train.AdamOptimizer()\n",
    "train_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES,\n",
    "                               scope=\"mnist*\")\n",
    "train_op = optimizer.minimize(loss, var_list=train_vars)\n",
    "\n",
    "match = tf.nn.in_top_k(logits, y, 1)\n",
    "accuracy = tf.reduce_mean(tf.cast(match, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['mnist_hidden/weights:0',\n",
       " 'mnist_hidden/BatchNorm/beta:0',\n",
       " 'mnist_hidden/BatchNorm/gamma:0',\n",
       " 'mnist_output/weights:0',\n",
       " 'mnist_output/BatchNorm/beta:0',\n",
       " 'mnist_output/BatchNorm/gamma:0']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[v.name for v in train_vars]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.41069 0.1875 0.170667\n",
      "0.785419 0.90625 0.86\n",
      "0.673975 0.9375 0.864\n",
      "0.613128 0.8125 0.884\n",
      "0.72141 0.875 0.874667\n",
      "0.641879 0.875 0.881333\n",
      "0.594866 0.90625 0.885333\n",
      "0.620639 0.90625 0.886667\n",
      "0.499118 0.96875 0.893333\n",
      "0.544461 0.875 0.886667\n",
      "0.400586 0.96875 0.896\n",
      "0.416376 0.96875 0.893333\n",
      "0.512343 0.9375 0.890667\n",
      "0.314586 1.0 0.902667\n",
      "0.350421 0.96875 0.886667\n",
      "0.347452 0.9375 0.894667\n",
      "0.52806 0.875 0.896\n",
      "0.404227 0.9375 0.892\n",
      "0.249052 0.96875 0.908\n",
      "0.354288 0.90625 0.909333\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    tf.global_variables_initializer().run() # initialize other variables\n",
    "    saver.restore(sess, \"./same_digit_model.ckpt\") # load existing variables\n",
    "    for e in range(n_epoches):\n",
    "        for i in range(5000 // batch_size):\n",
    "            X_batch, y_batch = next(train_batches)\n",
    "            sess.run(train_op, feed_dict={X: X_batch, y: y_batch, is_training:True})\n",
    "            if i % 100 == 0:\n",
    "                train_loss, train_acc = sess.run([loss, accuracy],\n",
    "                            feed_dict={X: X_batch, y: y_batch, is_training:False})\n",
    "                test_acc = sess.run(accuracy,\n",
    "                            feed_dict={X: X_test, y: y_test, is_training: False})\n",
    "                print(train_loss, train_acc, test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
