{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: seq2seq model with attention for language translation or chatbot?\n",
    "\n",
    "## some resources\n",
    "- [online tutorial](https://github.com/spro/practical-pytorch/blob/master/seq2seq-translation/seq2seq-translation-batched.ipynb) and [code](https://github.com/spro/practical-pytorch/tree/master/seq2seq-translation) from practical pytorch\n",
    "- MaximumEntropy [seq2seq-pytorch](https://github.com/MaximumEntropy/Seq2Seq-PyTorch)\n",
    "- IBM [pytorch seq2seq](https://github.com/IBM/pytorch-seq2seq)\n",
    "- [seq2seq.pytorch](https://github.com/eladhoffer/seq2seq.pytorch)\n",
    "- [seq2seq with tensorflow tutorials](https://github.com/ematvey/tensorflow-seq2seq-tutorials)\n",
    "- [seq2seq neural machine translation tutorial](https://github.com/tensorflow/nmt)\n",
    "- [chatbot based on seq2seq antilm](https://github.com/Marsan-Ma/tf_chatbot_seq2seq_antilm)\n",
    "- [practical seq2seq for chatbot](http://suriyadeepan.github.io/2016-12-31-practical-seq2seq/)\n",
    "\n",
    "## datasets\n",
    "- [Tab-delimited Bilingual Sentence Pairs](http://www.manythings.org/anki/)\n",
    "- [chat corpus](https://github.com/Marsan-Ma/chat_corpus)\n",
    "\n",
    "It might be too long to fit into one notebook, so split it into several."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.nn import functional as F\n",
    "from torch.utils import data\n",
    "from torch.autograd import Variable\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the language data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "PAD, SOS, EOS, UNK = 0, 1, 2, 3\n",
    "\n",
    "from collections import Counter\n",
    "import re\n",
    "\n",
    "# langauge model, aka w2i, i2w, and wc\n",
    "class Language(object):\n",
    "    def __init__(self, name, tokenizer):\n",
    "        self.tokenizer = tokenizer # regular expression\n",
    "        self.w2i = {\"PAD\":PAD, \"SOS\":SOS, \"EOS\": EOS, \"UNK\": UNK} # word to index\n",
    "        self.i2w = {PAD: \"PAD\", SOS: \"SOS\", EOS: \"EOS\", UNK: \"UNK\"} # index to word\n",
    "        self.wc = Counter() # word count\n",
    "        \n",
    "    def update(self, sentences):\n",
    "        for sentence in sentences:\n",
    "            words = re.findall(self.tokenizer, sentence.lower())\n",
    "            for word in words:\n",
    "                if word not in self.w2i:\n",
    "                    self.w2i[word] = len(self.w2i)\n",
    "                    self.i2w[len(self.i2w)] = word\n",
    "            self.wc.update(words)\n",
    "            \n",
    "    def prune(self, percent):\n",
    "        pass\n",
    "    \n",
    "English = Language(\"english\", u\"\\\\w+|\\S\")\n",
    "Chinese = Language(\"chinese\", u\".\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lines = open(\"/home/mali/ws/data/bilingual/cn2en/cmn.txt\").readlines()\n",
    "english_sents, chinese_sents = zip(*[line.split(\"\\t\") for line in lines])\n",
    "\n",
    "English.update(english_sents)\n",
    "Chinese.update(chinese_sents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "TODO: finish the language translation example. for now focus on simple case like string reversing for illustration and simplicity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reversed Seq Data as a simpler example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50000\n",
      "[7 4 9 0 0] [9 4 7 1 0 0] 3\n"
     ]
    }
   ],
   "source": [
    "## generate some data: \n",
    "## input - a sequence of integers(index), target: the reverse of it\n",
    "## for vocabulary setup, reserving index 0 for padding and index 1 for EOS\n",
    "\n",
    "## this corresponds to skipping the vocab building (word2inex, index2word) and\n",
    "## use index directly\n",
    "class ReverseSeqData(data.Dataset):\n",
    "    def __init__(self, vocab_size=10, max_seq=10, n_data=1000):\n",
    "        self.vocab_size = vocab_size\n",
    "        self.max_seq = max_seq\n",
    "        self.n_data = n_data\n",
    "        self.seqs = []\n",
    "        self.seq_lens = []\n",
    "        for _ in range(n_data):\n",
    "            seq_len = np.random.randint(2, max_seq)\n",
    "            seq = np.zeros(max_seq).astype(np.int64)\n",
    "            seq[:seq_len] = np.random.randint(2, 10, seq_len) # 0, 1 reserved for padding and EOS\n",
    "            self.seqs.append(seq)\n",
    "            self.seq_lens.append(seq_len)\n",
    "    def __len__(self):\n",
    "        return len(self.seqs)\n",
    "    def __getitem__(self, i):\n",
    "        seq = self.seqs[i]\n",
    "        seq_len = self.seq_lens[i]\n",
    "        target = np.zeros(self.max_seq + 1).astype(np.int64)\n",
    "        target[:seq_len+1] = np.array([x for x in seq[:seq_len][::-1]] + [1])\n",
    "        return (seq, target, seq_len)\n",
    "    \n",
    "toy_ds = ReverseSeqData(n_data=50000, max_seq=5)\n",
    "\n",
    "print(len(toy_ds))\n",
    "s, t, l = toy_ds[0]\n",
    "print(s, t, l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sort_seqs_by_len(*seqs, lens):\n",
    "    order = np.argsort(lens)[::-1]\n",
    "    sorted_seqs = []\n",
    "    for seq in seqs:\n",
    "        sorted_seqs.append(np.asarray(seq)[order])\n",
    "    return sorted_seqs + [np.asarray(lens)[order]]\n",
    "\n",
    "def get_seq_batches(ds, batch_size=128):\n",
    "    n = len(ds)\n",
    "    n_batches = n // batch_size\n",
    "    index = np.arange(n)\n",
    "    np.random.shuffle(index)\n",
    "    for bi in np.array_split(index, n_batches):\n",
    "        seqs, targets, lens = zip(*[toy_ds[i] for i in bi])\n",
    "        seqs, targets, lens = sort_seqs_by_len(seqs, targets, lens=lens)\n",
    "        x, y = torch.from_numpy(seqs), torch.from_numpy(targets)\n",
    "        yield x, y, lens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## seq2seq with `attention`\n",
    "- following the conventions in the literature, separate the implementations of encoder and decoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### encoder of attention model is usually just the normal encoder with\n",
    "- embedding layer\n",
    "- single/multiple layer of RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## use a one layer bidirectional RNN\n",
    "\n",
    "input_size = toy_ds.vocab_size\n",
    "embed_size = 8\n",
    "encoder_hidden_size = 16\n",
    "seq_len = toy_ds.max_seq\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.embedding = nn.Embedding(input_size, embed_size,\n",
    "                                      padding_idx=0)\n",
    "        self.rnn = nn.GRU(input_size=embed_size, hidden_size=encoder_hidden_size,\n",
    "                          batch_first=True, bidirectional=True)\n",
    "    def forward(self, x, seq_lens):\n",
    "        batch_size = x.size(0)\n",
    "        embed = self.embedding(x)\n",
    "        padded = pack_padded_sequence(embed, seq_lens, batch_first=True)\n",
    "        h0 = Variable(torch.zeros([2, batch_size, encoder_hidden_size])).cuda()\n",
    "        out, h = self.rnn(padded, h0)\n",
    "        out, _ = pad_packed_sequence(out, batch_first=True)\n",
    "        return out, h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([100, 4, 32]), torch.Size([2, 100, 16]))"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## test encoder\n",
    "m = Encoder().cuda()\n",
    "seqs, targets, lens = zip(*[toy_ds[i] for i in range(100)])\n",
    "seqs, targets, lens = sort_seqs_by_len(seqs, targets, lens=lens)\n",
    "\n",
    "x = Variable(torch.from_numpy(seqs)).cuda()\n",
    "out, h = m(x, lens)\n",
    "out.size(), h.size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### attention\n",
    "- it is a mechanism to weight the outputs from encoder, so later a weighted sum of these encoder outputs can be calculated as `context` vector\n",
    "- the `context` vector will be concated with decoder output at each step, going through a linear transform, to generate the final decoder output\n",
    "- the weights are usually normalized (softmax) version of some scores, which are essentially the similarity measure between each encoder output and current decoder output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "decoder_hidden_size = 12\n",
    "output_size = input_size # num of classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## attention, implemented as scoring current decoder output\n",
    "# with all encoder inputs projected to decoder space, based on their\n",
    "# inner product\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Attention, self).__init__()\n",
    "        self.encoder2decoder = nn.Linear(\n",
    "            encoder_hidden_size*2, # bidirectional\n",
    "            decoder_hidden_size)\n",
    "        \n",
    "    def forward(self, decoder_output, encoder_outputs):\n",
    "        \"\"\"\n",
    "        decoder_output: (batch_size, 1, decoder_hidden_size) of seq length 1\n",
    "        encoder_outputs: (batch_size, max_seq, 2*encoder_hidden_size) bidirectional\n",
    "        Returned scores should be of (batch_size, 1, max_seq), so that later they\n",
    "        can be weighted and sumed by batch matrix multiplication\n",
    "        \"\"\"\n",
    "        batch_size = encoder_outputs.size(0)\n",
    "        max_len = encoder_outputs.size(1)\n",
    "        # easier for softmax function for this shape\n",
    "        scores = Variable(torch.zeros([batch_size, max_len])).cuda()\n",
    "        \n",
    "        for b in range(batch_size):\n",
    "            for s in range(max_len):\n",
    "                scores[b, s] = self.score(decoder_output[b, 0, :], encoder_outputs[b, s, :])\n",
    "        \n",
    "        # cast it to the desired shape\n",
    "        scores = F.softmax(scores).unsqueeze(dim=1)\n",
    "        return scores\n",
    "    \n",
    "    def score(self, decoder_output, encoder_output):\n",
    "        projected_encoder = self.encoder2decoder(encoder_output.unsqueeze(dim=0)).squeeze()\n",
    "        return decoder_output.dot(projected_encoder)       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([100, 1, 4])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## test attention\n",
    "m = Attention().cuda()\n",
    "decoder_output = Variable(torch.randn([100, 1, decoder_hidden_size])).cuda()\n",
    "encoder_outputs = Variable(torch.randn([100, 4, encoder_hidden_size*2])).cuda()\n",
    "wts = m(decoder_output, encoder_outputs)\n",
    "wts.size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoder with attention\n",
    "- it generates sequence of outputs based on\n",
    "    - first input (e.g., SOS/EOS)\n",
    "    - first hidden state, from encoder last hidden\n",
    "    - encoder inputs: all inputs from encoder\n",
    "- at each step, \n",
    "    - embed previous output to input (ideally the same embed layer as encoder, but not necessary)\n",
    "    - get rnn output and hidden state\n",
    "    - calculate attenion (weights) based on current decoder output and encoder outputs, and get the context vector as the weighted sum\n",
    "    - concat the rnn_output and context_vector, project it, and get the final output (class probability) for the current step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class AttenDecoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        self.embedding = nn.Embedding(output_size,\n",
    "                                      embed_size, padding_idx=0)\n",
    "        self.rnn = nn.GRU(embed_size, decoder_hidden_size)\n",
    "        self.attn = Attention()\n",
    "        self.concat = nn.Linear(decoder_hidden_size*2,\n",
    "                                decoder_hidden_size)\n",
    "        self.out = nn.Linear(decoder_hidden_size, output_size)\n",
    "        \n",
    "    def forward(self, first_input, first_hidden, encoder_inputs):\n",
    "        batch_size = encoder_inputs.size(0)\n",
    "        max_seq = encoder_inputs.size(1)\n",
    "        \n",
    "        x, h = first_input, first_hidden\n",
    "        outs, hiddens, attentions = [], [], []\n",
    "        for s in range(max_seq):\n",
    "            embeded = self.embedding(x)\n",
    "            rnn_out, h = self.rnn(embeded, h)\n",
    "            attention = self.attn(rnn_out, encoder_inputs)\n",
    "            context = attention.bmm(encoder_inputs)\n",
    "            concat_in = torch.cat([rnn_out, context], dim=1)\n",
    "            concat_out = F.tanh(self.concat(concat_in))\n",
    "            out = self.out(concat_out)\n",
    "            x = out\n",
    "            \n",
    "            outs.append(out)\n",
    "            hiddens.append(h)\n",
    "            attentions.append(attention)\n",
    "            \n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "cannot assign module before Module.__init__() call",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-64-6f213c02cd93>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m## test decoder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAttenDecoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mfirst_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-63-36e7a2b2c1e8>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m         self.embedding = nn.Embedding(output_size,\n\u001b[0;32m----> 4\u001b[0;31m                                       embed_size, padding_idx=0)\n\u001b[0m\u001b[1;32m      5\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrnn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGRU\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membed_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder_hidden_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAttention\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/bin/anaconda3/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__setattr__\u001b[0;34m(self, name, value)\u001b[0m\n\u001b[1;32m    286\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mmodules\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    287\u001b[0m                     raise AttributeError(\n\u001b[0;32m--> 288\u001b[0;31m                         \"cannot assign module before Module.__init__() call\")\n\u001b[0m\u001b[1;32m    289\u001b[0m                 \u001b[0mremove_from\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__dict__\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_buffers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    290\u001b[0m                 \u001b[0mmodules\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: cannot assign module before Module.__init__() call"
     ]
    }
   ],
   "source": [
    "## test decoder\n",
    "m = AttenDecoder().cuda()\n",
    "first_input = Variable(torch.zeros([100]).long())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
