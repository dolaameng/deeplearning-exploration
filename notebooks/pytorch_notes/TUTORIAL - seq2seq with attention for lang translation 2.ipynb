{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: seq2seq model with attention for language translation or chatbot?\n",
    "\n",
    "## some resources\n",
    "- [online tutorial](https://github.com/spro/practical-pytorch/blob/master/seq2seq-translation/seq2seq-translation-batched.ipynb) and [code](https://github.com/spro/practical-pytorch/tree/master/seq2seq-translation) from practical pytorch\n",
    "- MaximumEntropy [seq2seq-pytorch](https://github.com/MaximumEntropy/Seq2Seq-PyTorch)\n",
    "- IBM [pytorch seq2seq](https://github.com/IBM/pytorch-seq2seq)\n",
    "- [seq2seq.pytorch](https://github.com/eladhoffer/seq2seq.pytorch)\n",
    "- [seq2seq with tensorflow tutorials](https://github.com/ematvey/tensorflow-seq2seq-tutorials)\n",
    "- [seq2seq neural machine translation tutorial](https://github.com/tensorflow/nmt)\n",
    "- [chatbot based on seq2seq antilm](https://github.com/Marsan-Ma/tf_chatbot_seq2seq_antilm)\n",
    "- [practical seq2seq for chatbot](http://suriyadeepan.github.io/2016-12-31-practical-seq2seq/)\n",
    "\n",
    "## datasets\n",
    "- [Tab-delimited Bilingual Sentence Pairs](http://www.manythings.org/anki/)\n",
    "- [chat corpus](https://github.com/Marsan-Ma/chat_corpus)\n",
    "\n",
    "It might be too long to fit into one notebook, so split it into several."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.nn import functional as F\n",
    "from torch.utils import data\n",
    "from torch.autograd import Variable\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the language data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "PAD, SOS, EOS, UNK = 0, 1, 2, 3\n",
    "\n",
    "from collections import Counter\n",
    "import re\n",
    "\n",
    "# langauge model, aka w2i, i2w, and wc\n",
    "class Language(object):\n",
    "    def __init__(self, name, tokenizer):\n",
    "        self.tokenizer = tokenizer # regular expression\n",
    "        self.w2i = {\"PAD\":PAD, \"SOS\":SOS, \"EOS\": EOS, \"UNK\": UNK} # word to index\n",
    "        self.i2w = {PAD: \"PAD\", SOS: \"SOS\", EOS: \"EOS\", UNK: \"UNK\"} # index to word\n",
    "        self.wc = Counter() # word count\n",
    "        \n",
    "    def update(self, sentences):\n",
    "        for sentence in sentences:\n",
    "            words = re.findall(self.tokenizer, sentence.lower())\n",
    "            for word in words:\n",
    "                if word not in self.w2i:\n",
    "                    self.w2i[word] = len(self.w2i)\n",
    "                    self.i2w[len(self.i2w)] = word\n",
    "            self.wc.update(words)\n",
    "            \n",
    "    def prune(self, percent):\n",
    "        pass\n",
    "    \n",
    "English = Language(\"english\", u\"\\\\w+|\\S\")\n",
    "Chinese = Language(\"chinese\", u\".\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lines = open(\"/home/mali/ws/data/bilingual/cn2en/cmn.txt\").readlines()\n",
    "english_sents, chinese_sents = zip(*[line.split(\"\\t\") for line in lines])\n",
    "\n",
    "English.update(english_sents)\n",
    "Chinese.update(chinese_sents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "TODO: finish the language translation example. for now focus on simple case like string reversing for illustration and simplicity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reversed Seq Data as a simpler example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50000\n",
      "[5 9 2 3 0 0] [3 2 9 5 1 0 0] 4\n"
     ]
    }
   ],
   "source": [
    "## generate some data: \n",
    "## input - a sequence of integers(index), target: the reverse of it\n",
    "## for vocabulary setup, reserving index 0 for padding and index 1 for EOS\n",
    "\n",
    "## this corresponds to skipping the vocab building (word2inex, index2word) and\n",
    "## use index directly\n",
    "class ReverseSeqData(data.Dataset):\n",
    "    def __init__(self, vocab_size=10, max_seq=10, n_data=1000):\n",
    "        self.vocab_size = vocab_size\n",
    "        self.max_seq = max_seq\n",
    "        self.n_data = n_data\n",
    "        self.seqs = []\n",
    "        self.seq_lens = []\n",
    "        for _ in range(n_data):\n",
    "            seq_len = np.random.randint(2, max_seq)\n",
    "            seq = np.zeros(max_seq).astype(np.int64)\n",
    "            seq[:seq_len] = np.random.randint(2, 10, seq_len) # 0, 1 reserved for padding and EOS\n",
    "            self.seqs.append(seq)\n",
    "            self.seq_lens.append(seq_len)\n",
    "    def __len__(self):\n",
    "        return len(self.seqs)\n",
    "    def __getitem__(self, i):\n",
    "        seq = self.seqs[i]\n",
    "        seq_len = self.seq_lens[i]\n",
    "        target = np.zeros(self.max_seq + 1).astype(np.int64)\n",
    "        target[:seq_len+1] = np.array([x for x in seq[:seq_len][::-1]] + [1])\n",
    "        return (seq, target, seq_len)\n",
    "    \n",
    "toy_ds = ReverseSeqData(n_data=50000, max_seq=6)\n",
    "\n",
    "print(len(toy_ds))\n",
    "s, t, l = toy_ds[0]\n",
    "print(s, t, l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sort_seqs_by_len(*seqs, lens):\n",
    "    order = np.argsort(lens)[::-1]\n",
    "    sorted_seqs = []\n",
    "    for seq in seqs:\n",
    "        sorted_seqs.append(np.asarray(seq)[order])\n",
    "    return sorted_seqs + [np.asarray(lens)[order]]\n",
    "\n",
    "def get_seq_batches(ds, batch_size=128):\n",
    "    n = len(ds)\n",
    "    n_batches = n // batch_size\n",
    "    index = np.arange(n)\n",
    "    np.random.shuffle(index)\n",
    "    for bi in np.array_split(index, n_batches):\n",
    "        seqs, targets, lens = zip(*[toy_ds[i] for i in bi])\n",
    "        seqs, targets, lens = sort_seqs_by_len(seqs, targets, lens=lens)\n",
    "        x, y = torch.from_numpy(seqs), torch.from_numpy(targets)\n",
    "        yield x, y, lens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## seq2seq with `attention`\n",
    "- following the conventions in the literature, separate the implementations of encoder and decoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### encoder of attention model is usually just the normal encoder with\n",
    "- embedding layer\n",
    "- single/multiple layer of RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## use a one layer bidirectional RNN\n",
    "\n",
    "input_size = toy_ds.vocab_size\n",
    "embed_size = 8\n",
    "encoder_hidden_size = 16\n",
    "input_seq_len = toy_ds.max_seq\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.embedding = nn.Embedding(input_size, embed_size,\n",
    "                                      padding_idx=0)\n",
    "        self.rnn = nn.GRU(input_size=embed_size, hidden_size=encoder_hidden_size,\n",
    "                          batch_first=True, bidirectional=True)\n",
    "    def forward(self, x, seq_lens):\n",
    "        batch_size = x.size(0)\n",
    "        embed = self.embedding(x)\n",
    "        padded = pack_padded_sequence(embed, seq_lens, batch_first=True)\n",
    "        h0 = Variable(torch.zeros([2, batch_size, encoder_hidden_size])).cuda()\n",
    "        out, h = self.rnn(padded, h0)\n",
    "        out, _ = pad_packed_sequence(out, batch_first=True)\n",
    "        h = torch.cat([h[1:, ...], h[1:, ...]], dim=-1)\n",
    "        return out, h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([100, 5, 32]), torch.Size([1, 100, 32]))"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## test encoder\n",
    "m = Encoder().cuda()\n",
    "seqs, targets, lens = zip(*[toy_ds[i] for i in range(100)])\n",
    "seqs, targets, lens = sort_seqs_by_len(seqs, targets, lens=lens)\n",
    "\n",
    "x = Variable(torch.from_numpy(seqs)).cuda()\n",
    "out, h = m(x, lens)\n",
    "out.size(), h.size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### attention\n",
    "- it is a mechanism to weight the outputs from encoder, so later a weighted sum of these encoder outputs can be calculated as `context` vector\n",
    "- the `context` vector will be concated with decoder output at each step, going through a linear transform, to generate the final decoder output\n",
    "- the weights are usually normalized (softmax) version of some scores, which are essentially the similarity measure between each encoder output and current decoder output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "decoder_hidden_size = encoder_hidden_size * 2\n",
    "output_size = input_size # num of classes\n",
    "output_seq_len = input_seq_len + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## attention, implemented as scoring current decoder output\n",
    "# with all encoder inputs projected to decoder space, based on their\n",
    "# inner product\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Attention, self).__init__()\n",
    "        self.encoder2decoder = nn.Linear(\n",
    "            encoder_hidden_size*2, # bidirectional\n",
    "            decoder_hidden_size)\n",
    "        \n",
    "    def forward(self, decoder_output, encoder_outputs):\n",
    "        \"\"\"\n",
    "        decoder_output: (batch_size, 1, decoder_hidden_size) of seq length 1\n",
    "        encoder_outputs: (batch_size, max_seq, 2*encoder_hidden_size) bidirectional\n",
    "        Returned scores should be of (batch_size, 1, max_seq), so that later they\n",
    "        can be weighted and sumed by batch matrix multiplication\n",
    "        \"\"\"\n",
    "        batch_size = encoder_outputs.size(0)\n",
    "        max_len = encoder_outputs.size(1)\n",
    "        # easier for softmax function for this shape\n",
    "        scores = Variable(torch.zeros([batch_size, max_len])).cuda()\n",
    "        \n",
    "        for b in range(batch_size):\n",
    "            for s in range(max_len):\n",
    "                scores[b, s] = self.score(decoder_output[b, 0, :], encoder_outputs[b, s, :])\n",
    "        \n",
    "        # cast it to the desired shape\n",
    "        scores = F.softmax(scores).unsqueeze(dim=1)\n",
    "        return scores\n",
    "    \n",
    "    def score(self, decoder_output, encoder_output):\n",
    "        projected_encoder = self.encoder2decoder(encoder_output.unsqueeze(dim=0)).squeeze()\n",
    "        return decoder_output.dot(projected_encoder)       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([100, 1, 4])"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## test attention\n",
    "m = Attention().cuda()\n",
    "decoder_output = Variable(torch.randn([100, 1, decoder_hidden_size])).cuda()\n",
    "encoder_outputs = Variable(torch.randn([100, 4, encoder_hidden_size*2])).cuda()\n",
    "wts = m(decoder_output, encoder_outputs)\n",
    "wts.size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoder with attention\n",
    "- it generates sequence of outputs based on\n",
    "    - first input (e.g., SOS/EOS)\n",
    "    - first hidden state, from encoder last hidden\n",
    "    - encoder inputs: all inputs from encoder\n",
    "- at each step, \n",
    "    - embed previous output to input (ideally the same embed layer as encoder, but not necessary)\n",
    "    - get rnn output and hidden state\n",
    "    - calculate attenion (weights) based on current decoder output and encoder outputs, and get the context vector as the weighted sum\n",
    "    - concat the rnn_output and context_vector, project it, and get the final output (class probability) for the current step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class AttenDecoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(AttenDecoder, self).__init__()\n",
    "        self.embedding = nn.Embedding(output_size,\n",
    "                                      embed_size, padding_idx=0)\n",
    "        self.rnn = nn.GRU(embed_size, decoder_hidden_size, batch_first=True)\n",
    "        self.attn = Attention()\n",
    "        self.concat = nn.Linear(decoder_hidden_size*2,\n",
    "                                decoder_hidden_size)\n",
    "        self.out = nn.Linear(decoder_hidden_size, output_size)\n",
    "        \n",
    "    def forward(self, first_input, first_hidden, encoder_inputs):\n",
    "        batch_size = encoder_inputs.size(0)\n",
    "        max_seq = encoder_inputs.size(1)\n",
    "        \n",
    "        x, h = first_input, first_hidden\n",
    "        outs, hiddens, attentions = [], [], []\n",
    "        for s in range(output_seq_len): \n",
    "            embeded = self.embedding(x) #->(B, 1, H)\n",
    "            rnn_out, h = self.rnn(embeded, h) #-> (B, 1, H), (1, B, H)\n",
    "            attention = self.attn(rnn_out, encoder_inputs) # (B, 1, S)\n",
    "            context = attention.bmm(encoder_inputs) #->(B, 1, H)\n",
    "            concat_in = torch.cat([rnn_out, context], dim=-1).squeeze() #->(B, 2H)\n",
    "            concat_out = F.tanh(self.concat(concat_in)) #->(B, H)\n",
    "            out = self.out(concat_out) #->(B, C)\n",
    "            _, x = out.max(dim=1)\n",
    "            x = x.unsqueeze(dim=1) #->(B, )\n",
    "            \n",
    "            outs.append(out)\n",
    "            attentions.append(attention)\n",
    "        \n",
    "        final_out = torch.stack(outs, dim=1) #->(B, S, C)\n",
    "        final_attention = torch.stack(attentions, dim=2).squeeze() #->(B, output_seq, input_seq)\n",
    "        return final_out, final_attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([100, 7, 10]), torch.Size([100, 7, 7]))"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## test decoder\n",
    "m = AttenDecoder().cuda()\n",
    "first_input = Variable(torch.zeros([100, 1]).long()).cuda()\n",
    "first_hidden = Variable(torch.randn([1, 100, decoder_hidden_size])).cuda()\n",
    "encoder_inputs = Variable(torch.randn([100, output_seq_len, 2*encoder_hidden_size])).cuda()\n",
    "\n",
    "out, attention = m(first_input, first_hidden, encoder_inputs)\n",
    "out.size(), attention.size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### seq2seq by putting encoder and decoder together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Seq2Seq, self).__init__()\n",
    "        self.encoder = Encoder()\n",
    "        self.decoder = AttenDecoder()\n",
    "    def forward(self, x, seq_lens):\n",
    "        batch_size = x.size(0)\n",
    "        encoder_outs, encoder_h = self.encoder(x, seq_lens)\n",
    "        \n",
    "        first_input = Variable(torch.zeros([batch_size, 1])).cuda().long()\n",
    "        decoder_outs, attentions = self.decoder(first_input, encoder_h, encoder_outs)\n",
    "        \n",
    "        return decoder_outs, attentions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# test the model\n",
    "seqs, targets, lens = zip(*[toy_ds[i] for i in range(100)])\n",
    "seqs, targets, lens = sort_seqs_by_len(seqs, targets, lens=lens)\n",
    "\n",
    "x = Variable(torch.from_numpy(seqs)).cuda()\n",
    "\n",
    "m = Seq2Seq().cuda()\n",
    "y, attentions = m(x, lens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "n_epochs = 2\n",
    "\n",
    "model = Seq2Seq().cuda()\n",
    "model.train()\n",
    "\n",
    "\n",
    "\n",
    "objective = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0 2.310865879058838\n",
      "0 50 1.825005054473877\n",
      "0 100 1.240592122077942\n",
      "0 150 0.8559282422065735\n",
      "0 200 0.5695987343788147\n",
      "0 250 0.2938241958618164\n",
      "0 300 0.14886365830898285\n",
      "0 350 0.06300819665193558\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-83-a27d6175d762>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_cell_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'time'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'for epoch in range(n_epochs):\\n    batches = get_seq_batches(toy_ds, batch_size=batch_size)\\n    for b, (bx, by, lens) in enumerate(batches):\\n        x = Variable(bx).cuda()\\n        y = Variable(by).cuda()\\n        yhat, attentions = model(x, lens)\\n        loss = objective(yhat.view([-1, 10]), y.view([-1]))\\n        \\n        optimizer.zero_grad()\\n        loss.backward()\\n        optimizer.step()\\n        \\n        if b % 50 == 0:\\n            print(epoch, b, loss.data[0])'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/bin/anaconda3/lib/python3.6/site-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mrun_cell_magic\u001b[0;34m(self, magic_name, line, cell)\u001b[0m\n\u001b[1;32m   2113\u001b[0m             \u001b[0mmagic_arg_s\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvar_expand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstack_depth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2114\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuiltin_trap\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2115\u001b[0;31m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmagic_arg_s\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2116\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<decorator-gen-59>\u001b[0m in \u001b[0;36mtime\u001b[0;34m(self, line, cell, local_ns)\u001b[0m\n",
      "\u001b[0;32m/usr/bin/anaconda3/lib/python3.6/site-packages/IPython/core/magic.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(f, *a, **k)\u001b[0m\n\u001b[1;32m    186\u001b[0m     \u001b[0;31m# but it's overkill for just that one bit of state.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mmagic_deco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 188\u001b[0;31m         \u001b[0mcall\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    189\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    190\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/bin/anaconda3/lib/python3.6/site-packages/IPython/core/magics/execution.py\u001b[0m in \u001b[0;36mtime\u001b[0;34m(self, line, cell, local_ns)\u001b[0m\n\u001b[1;32m   1183\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1184\u001b[0m             \u001b[0mst\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclock2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1185\u001b[0;31m             \u001b[0mexec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mglob\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocal_ns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1186\u001b[0m             \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclock2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1187\u001b[0m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m/usr/bin/anaconda3/lib/python3.6/site-packages/torch/autograd/variable.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, retain_variables)\u001b[0m\n\u001b[1;32m    154\u001b[0m                 \u001b[0mVariable\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m         \"\"\"\n\u001b[0;32m--> 156\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_variables\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    157\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/bin/anaconda3/lib/python3.6/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(variables, grad_variables, retain_graph, create_graph, retain_variables)\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[0;32m---> 98\u001b[0;31m         variables, grad_variables, retain_graph)\n\u001b[0m\u001b[1;32m     99\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for epoch in range(n_epochs):\n",
    "    batches = get_seq_batches(toy_ds, batch_size=batch_size)\n",
    "    for b, (bx, by, lens) in enumerate(batches):\n",
    "        x = Variable(bx).cuda()\n",
    "        y = Variable(by).cuda()\n",
    "        yhat, attentions = model(x, lens)\n",
    "        loss = objective(yhat.view([-1, 10]), y.view([-1]))\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if b % 50 == 0:\n",
    "            print(epoch, b, loss.data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "## evaluation\n",
    "model.eval()\n",
    "seqs, targets, lens = zip(*[toy_ds[i] for i in range(20)])\n",
    "seqs, targets, lens = sort_seqs_by_len(seqs, targets, lens=lens)\n",
    "\n",
    "x = Variable(torch.from_numpy(seqs)).cuda()\n",
    "y, attentions = model(x, lens)\n",
    "_, label = torch.max(y, dim=-1)\n",
    "print(\"accuracy:\", np.mean(label.data.cpu().numpy() == targets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "atten_data = attentions.cpu().data.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sb\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "cm = pd.DataFrame(atten_data[0], index=label.data.cpu().numpy()[0], columns=seqs[0][:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/bin/anaconda3/lib/python3.6/site-packages/matplotlib/font_manager.py:1297: UserWarning: findfont: Font family ['sans-serif'] not found. Falling back to DejaVu Sans\n",
      "  (prop.get_family(), self.defaultFamily[fontext]))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7f011fbc0710>"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAV0AAAD4CAYAAABPLjVeAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nDTLpSlKDTLqS1KD/A7JRqGAg+PmjAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f011fbeac88>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sb.heatmap(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
