{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: seq2seq model with attention for language translation or chatbot?\n",
    "\n",
    "## some resources\n",
    "- [online tutorial](https://github.com/spro/practical-pytorch/blob/master/seq2seq-translation/seq2seq-translation-batched.ipynb) and [code](https://github.com/spro/practical-pytorch/tree/master/seq2seq-translation) from practical pytorch\n",
    "- MaximumEntropy [seq2seq-pytorch](https://github.com/MaximumEntropy/Seq2Seq-PyTorch)\n",
    "- IBM [pytorch seq2seq](https://github.com/IBM/pytorch-seq2seq)\n",
    "- [seq2seq.pytorch](https://github.com/eladhoffer/seq2seq.pytorch)\n",
    "- [seq2seq with tensorflow tutorials](https://github.com/ematvey/tensorflow-seq2seq-tutorials)\n",
    "- [seq2seq neural machine translation tutorial](https://github.com/tensorflow/nmt)\n",
    "- [chatbot based on seq2seq antilm](https://github.com/Marsan-Ma/tf_chatbot_seq2seq_antilm)\n",
    "- [practical seq2seq for chatbot](http://suriyadeepan.github.io/2016-12-31-practical-seq2seq/)\n",
    "\n",
    "## datasets\n",
    "- [Tab-delimited Bilingual Sentence Pairs](http://www.manythings.org/anki/)\n",
    "- [chat corpus](https://github.com/Marsan-Ma/chat_corpus)\n",
    "\n",
    "It might be too long to fit into one notebook, so split it into several."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.nn import functional as F\n",
    "from torch.utils import data\n",
    "from torch.autograd import Variable\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Vanila basic seq2seq model\n",
    "### encoder: \n",
    "- a simple rnn (GRU/LSTM), some times with embedding layer before it\n",
    "- don't care about the output, instead, just take the last hidden state (called thought vector or context?)\n",
    "- input: batch of padded sequences (if of varying lengths), size=(batch_size, seq_len, input_dim) or (seq_len, batch_size, input_dim) depending on whether it is time_major or batch_major\n",
    "- output: the hidden state at the last step, size=(batch_size, hidden_dim)\n",
    "\n",
    "### decoder\n",
    "- a simple rnn, with a projection layer (softmax) after it, to map rnn seq output to vocab classification\n",
    "- the initial hidden state will be the thought vector, aka the last hidden state from encoder\n",
    "- for each step, the input should be the output from last step. And the input of first step for decoder will be a special mark, e.g., SOS (start of sentence) or just EOS\n",
    "- the sequence output will be projected by another one/sevearl layers to map them to class probablities\n",
    "\n",
    "### example\n",
    "- I will follow this [tutorial](https://github.com/ematvey/tensorflow-seq2seq-tutorials/blob/master/1-seq2seq.ipynb), trying to reverse the sequence by using a seq2seq model, in pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50000\n",
      "[4 7 6 0 0] [6 7 4 1 0 0] 3\n"
     ]
    }
   ],
   "source": [
    "## generate some data: \n",
    "## input - a sequence of integers(index), target: the reverse of it\n",
    "## for vocabulary setup, reserving index 0 for padding and index 1 for EOS\n",
    "\n",
    "## this corresponds to skipping the vocab building (word2inex, index2word) and\n",
    "## use index directly\n",
    "class ReverseSeqData(data.Dataset):\n",
    "    def __init__(self, vocab_size=10, max_seq=10, n_data=1000):\n",
    "        self.vocab_size = vocab_size\n",
    "        self.max_seq = max_seq\n",
    "        self.n_data = n_data\n",
    "        self.seqs = []\n",
    "        self.seq_lens = []\n",
    "        for _ in range(n_data):\n",
    "            seq_len = np.random.randint(2, max_seq)\n",
    "            seq = np.zeros(max_seq).astype(np.int64)\n",
    "            seq[:seq_len] = np.random.randint(2, 10, seq_len) # 0, 1 reserved for padding and EOS\n",
    "            self.seqs.append(seq)\n",
    "            self.seq_lens.append(seq_len)\n",
    "    def __len__(self):\n",
    "        return len(self.seqs)\n",
    "    def __getitem__(self, i):\n",
    "        seq = self.seqs[i]\n",
    "        seq_len = self.seq_lens[i]\n",
    "        target = np.zeros(self.max_seq + 1).astype(np.int64)\n",
    "        target[:seq_len+1] = np.array([x for x in seq[:seq_len][::-1]] + [1])\n",
    "        return (seq, target, seq_len)\n",
    "    \n",
    "toy_ds = ReverseSeqData(n_data=50000, max_seq=5)\n",
    "\n",
    "print(len(toy_ds))\n",
    "s, t, l = toy_ds[0]\n",
    "print(s, t, l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sort_seqs_by_len(*seqs, lens):\n",
    "    order = np.argsort(lens)[::-1]\n",
    "    sorted_seqs = []\n",
    "    for seq in seqs:\n",
    "        sorted_seqs.append(np.asarray(seq)[order])\n",
    "    return sorted_seqs + [np.asarray(lens)[order]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## model\n",
    "\n",
    "vector_dim = 8\n",
    "vocab_size = toy_ds.vocab_size\n",
    "\n",
    "class BasicSeq2Seq(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(BasicSeq2Seq, self).__init__()\n",
    "        self.embed = nn.Embedding(vocab_size, vector_dim, padding_idx=0)\n",
    "        self.encode = nn.GRU(input_size=8, hidden_size=vector_dim, num_layers=1, batch_first=True)\n",
    "        self.decode = nn.GRU(input_size=8, hidden_size=vector_dim, num_layers=1, batch_first=True)\n",
    "        self.project = nn.Linear(vector_dim, vocab_size)\n",
    "        \n",
    "    def forward(self, seqs, seq_lens):\n",
    "        batch_size = seqs.size(0)\n",
    "        target_seq_len = seqs.size(1) + 1\n",
    "        embeded = self.embed(seqs)\n",
    "        \n",
    "        padded = pack_padded_sequence(embeded, seq_lens, batch_first=True)\n",
    "        h0 = Variable(torch.zeros([1, batch_size, vector_dim])).cuda()\n",
    "        _, h = self.encode(padded, h0)\n",
    "        \n",
    "        ys = []\n",
    "        # first input to decoder is EOS, which is 1 in index\n",
    "        y = Variable(torch.ones([batch_size, 1])).long().cuda()\n",
    "        y = self.embed(y)\n",
    "        for i in range(target_seq_len):\n",
    "            y, h = self.decode(y, h)\n",
    "            ys.append(y)\n",
    "        out = torch.cat(ys, dim=1)\n",
    "        \n",
    "        logits = self.project(out.view([-1, vector_dim]))\n",
    "        return logits.view([batch_size, target_seq_len, vocab_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((16, 5), (16, 6), torch.Size([16, 6, 10]))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## test model\n",
    "seqs, targets, lens = zip(*[toy_ds[i] for i in range(16)])\n",
    "# seqs = np.array(seqs)\n",
    "# targets = np.array(targets)\n",
    "# i = np.argsort(lens)[::-1]\n",
    "# seqs = seqs[i]\n",
    "# targets = targets[i]\n",
    "# lens = np.array(lens)[i]\n",
    "seqs, targets, lens = sort_seqs_by_len(seqs, targets, lens=lens)\n",
    "\n",
    "x = Variable(torch.from_numpy(seqs)).cuda()\n",
    "m = BasicSeq2Seq().cuda()\n",
    "y = m(x, lens)\n",
    "seqs.shape, targets.shape, y.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## training\n",
    "batch_size = 128\n",
    "n_batches = len(toy_ds) // batch_size\n",
    "n_epochs = 45\n",
    "\n",
    "objective = nn.CrossEntropyLoss()\n",
    "\n",
    "model = BasicSeq2Seq().cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0 2.315696954727173\n",
      "0 195 1.6249631643295288\n",
      "15 0 0.23702576756477356\n",
      "15 195 0.21347297728061676\n",
      "30 0 0.04112502932548523\n",
      "30 195 0.03904372826218605\n",
      "45 0 0.00453186733648181\n",
      "45 195 0.00397515669465065\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-24ae7ed05316>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_cell_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'time'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'model.train()\\n\\noptimizer = optim.Adam(model.parameters())\\nindex = np.arange(0, len(toy_ds))\\n\\nfor epoch in range(n_epochs):\\n    \\n    np.random.shuffle(index)\\n    for b, bi in enumerate(np.array_split(index, n_batches)):\\n        seqs, targets, lens = zip(*[toy_ds[i] for i in bi])\\n        seqs, targets, lens = sort_seqs_by_len(seqs, targets, lens=lens)\\n        \\n        x = Variable(torch.from_numpy(seqs)).cuda()\\n        y = Variable(torch.from_numpy(targets)).cuda()\\n        logits = model(x, lens)\\n        \\n        loss = objective(logits.view([-1, vocab_size]), y.view([-1]))\\n        model.zero_grad()\\n        loss.backward()\\n        optimizer.step()\\n        \\n        if epoch % 15 == 0 and b % (n_batches//2) == 0:\\n            print(epoch, b, loss.data[0])'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/bin/anaconda3/lib/python3.6/site-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mrun_cell_magic\u001b[0;34m(self, magic_name, line, cell)\u001b[0m\n\u001b[1;32m   2113\u001b[0m             \u001b[0mmagic_arg_s\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvar_expand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstack_depth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2114\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuiltin_trap\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2115\u001b[0;31m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmagic_arg_s\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2116\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<decorator-gen-59>\u001b[0m in \u001b[0;36mtime\u001b[0;34m(self, line, cell, local_ns)\u001b[0m\n",
      "\u001b[0;32m/usr/bin/anaconda3/lib/python3.6/site-packages/IPython/core/magic.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(f, *a, **k)\u001b[0m\n\u001b[1;32m    186\u001b[0m     \u001b[0;31m# but it's overkill for just that one bit of state.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mmagic_deco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 188\u001b[0;31m         \u001b[0mcall\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    189\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    190\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/bin/anaconda3/lib/python3.6/site-packages/IPython/core/magics/execution.py\u001b[0m in \u001b[0;36mtime\u001b[0;34m(self, line, cell, local_ns)\u001b[0m\n\u001b[1;32m   1183\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1184\u001b[0m             \u001b[0mst\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclock2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1185\u001b[0;31m             \u001b[0mexec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mglob\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocal_ns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1186\u001b[0m             \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclock2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1187\u001b[0m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m/usr/bin/anaconda3/lib/python3.6/site-packages/torch/autograd/variable.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, retain_variables)\u001b[0m\n\u001b[1;32m    154\u001b[0m                 \u001b[0mVariable\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m         \"\"\"\n\u001b[0;32m--> 156\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_variables\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    157\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/bin/anaconda3/lib/python3.6/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(variables, grad_variables, retain_graph, create_graph, retain_variables)\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[0;32m---> 98\u001b[0;31m         variables, grad_variables, retain_graph)\n\u001b[0m\u001b[1;32m     99\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%%time\n",
    "model.train()\n",
    "\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "index = np.arange(0, len(toy_ds))\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    \n",
    "    np.random.shuffle(index)\n",
    "    for b, bi in enumerate(np.array_split(index, n_batches)):\n",
    "        seqs, targets, lens = zip(*[toy_ds[i] for i in bi])\n",
    "        seqs, targets, lens = sort_seqs_by_len(seqs, targets, lens=lens)\n",
    "        \n",
    "        x = Variable(torch.from_numpy(seqs)).cuda()\n",
    "        y = Variable(torch.from_numpy(targets)).cuda()\n",
    "        logits = model(x, lens)\n",
    "        \n",
    "        loss = objective(logits.view([-1, vocab_size]), y.view([-1]))\n",
    "        model.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if epoch % 15 == 0 and b % (n_batches//2) == 0:\n",
    "            print(epoch, b, loss.data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "## evaluation\n",
    "model.eval()\n",
    "seqs, targets, lens = zip(*[toy_ds[i] for i in range(20)])\n",
    "seqs, targets, lens = sort_seqs_by_len(seqs, targets, lens=lens)\n",
    "\n",
    "x = Variable(torch.from_numpy(seqs)).cuda()\n",
    "y = model(x, lens)\n",
    "_, label = torch.max(y, dim=-1)\n",
    "print(\"accuracy:\", np.mean(label.data.cpu().numpy() == targets))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Seq2seq model with bidirectional encoder\n",
    "- everything else is the same as the basic model, except that the encoder now is using a bidirectional rnn\n",
    "- concat the hidden state from both directions and use it as the initial state for decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## model\n",
    "\n",
    "vector_dim = 8\n",
    "vocab_size = toy_ds.vocab_size\n",
    "\n",
    "class BidiSeq2Seq(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(BidiSeq2Seq, self).__init__()\n",
    "        self.embed = nn.Embedding(vocab_size, vector_dim, padding_idx=0)\n",
    "        self.encode = nn.GRU(input_size=8, hidden_size=vector_dim, num_layers=1,\n",
    "                             batch_first=True, bidirectional=True)\n",
    "        # decoder has double hidden dimension to accomodate bidirectional state from encoder\n",
    "        self.decode = nn.GRU(input_size=8, hidden_size=vector_dim*2, num_layers=1, batch_first=True)\n",
    "        # project layer to bring the output of decoder to dimension = its input\n",
    "        self.project = nn.Linear(vector_dim*2, vector_dim)\n",
    "        self.classify = nn.Linear(vector_dim, vocab_size)\n",
    "        \n",
    "    def forward(self, seqs, seq_lens):\n",
    "        batch_size = seqs.size(0)\n",
    "        target_seq_len = seqs.size(1) + 1\n",
    "        embeded = self.embed(seqs)\n",
    "        \n",
    "        padded = pack_padded_sequence(embeded, seq_lens, batch_first=True)\n",
    "        h0 = Variable(torch.zeros([2, batch_size, vector_dim])).cuda()\n",
    "        _, h = self.encode(padded, h0, )\n",
    "        h = torch.cat([h[0,...], h[1,...]], dim=1).unsqueeze(dim=0)\n",
    "        \n",
    "        ys = []\n",
    "        # first input to decoder is EOS, which is 1 in index\n",
    "        y = Variable(torch.ones([batch_size, 1])).long().cuda()\n",
    "        y = self.embed(y)\n",
    "        for i in range(target_seq_len):\n",
    "            y, h = self.decode(y, h)\n",
    "            y = self.project(y)\n",
    "            y = F.elu(y)\n",
    "            ys.append(y)\n",
    "        out = torch.cat(ys, dim=1)\n",
    "        \n",
    "        logits = self.classify(out.view([-1, vector_dim]))\n",
    "        return logits.view([batch_size, target_seq_len, vocab_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((16, 5), (16, 6), torch.Size([16, 6, 10]))"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## test model\n",
    "seqs, targets, lens = zip(*[toy_ds[i] for i in range(16)])\n",
    "seqs, targets, lens = sort_seqs_by_len(seqs, targets, lens=lens)\n",
    "\n",
    "x = Variable(torch.from_numpy(seqs)).cuda()\n",
    "m = BidiSeq2Seq().cuda()\n",
    "y = m(x, lens)\n",
    "seqs.shape, targets.shape, y.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## training\n",
    "\n",
    "## make it a little more challenging by using a longer max_seq\n",
    "toy_ds = ReverseSeqData(n_data=50000, max_seq=7)\n",
    "\n",
    "batch_size = 128\n",
    "n_batches = len(toy_ds) // batch_size\n",
    "n_epochs = 30\n",
    "\n",
    "objective = nn.CrossEntropyLoss()\n",
    "\n",
    "model = BidiSeq2Seq().cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0 2.363103151321411\n",
      "0 195 1.2488439083099365\n",
      "15 0 0.074092335999012\n",
      "15 195 0.09117448329925537\n",
      "CPU times: user 14min 16s, sys: 2min 5s, total: 16min 22s\n",
      "Wall time: 16min 25s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "model.train()\n",
    "\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "index = np.arange(0, len(toy_ds))\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    \n",
    "    np.random.shuffle(index)\n",
    "    for b, bi in enumerate(np.array_split(index, n_batches)):\n",
    "        seqs, targets, lens = zip(*[toy_ds[i] for i in bi])\n",
    "        seqs, targets, lens = sort_seqs_by_len(seqs, targets, lens=lens)\n",
    "        \n",
    "        x = Variable(torch.from_numpy(seqs)).cuda()\n",
    "        y = Variable(torch.from_numpy(targets)).cuda()\n",
    "        logits = model(x, lens)\n",
    "        \n",
    "        loss = objective(logits.view([-1, vocab_size]), y.view([-1]))\n",
    "        model.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if epoch % 15 == 0 and b % (n_batches//2) == 0:\n",
    "            print(epoch, b, loss.data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "## evaluation\n",
    "model.eval()\n",
    "seqs, targets, lens = zip(*[toy_ds[i] for i in range(20)])\n",
    "seqs, targets, lens = sort_seqs_by_len(seqs, targets, lens=lens)\n",
    "\n",
    "x = Variable(torch.from_numpy(seqs)).cuda()\n",
    "y = model(x, lens)\n",
    "_, label = torch.max(y, dim=-1)\n",
    "print(\"accuracy:\", np.mean(label.data.cpu().numpy() == targets))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## another popular way of doing decoder\n",
    "- the direct output of the decoder is of dim (hidden_dim), to map it back to input_dim, and make it more theoritically correct, we do the following:\n",
    "    - map the output to output space using softmax\n",
    "    - get the most likely prediction (or sampling in general)\n",
    "    - get the embedding of the prediction as the new input\n",
    "- another advantage is that now embeddings for both encoder and decoder are the same"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## model\n",
    "\n",
    "vector_dim = 8\n",
    "vocab_size = toy_ds.vocab_size\n",
    "\n",
    "class BidiSeq2Seq(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(BidiSeq2Seq, self).__init__()\n",
    "        self.embed = nn.Embedding(vocab_size, vector_dim, padding_idx=0)\n",
    "        self.encode = nn.GRU(input_size=8, hidden_size=vector_dim, num_layers=1,\n",
    "                             batch_first=True, bidirectional=True)\n",
    "        # decoder has double hidden dimension to accomodate bidirectional state from encoder\n",
    "        self.decode = nn.GRU(input_size=8, hidden_size=vector_dim*2, num_layers=1, batch_first=True)\n",
    "        # project layer to bring the output of decoder to dimension = its input\n",
    "        self.project = nn.Linear(vector_dim*2, vocab_size)\n",
    "        \n",
    "    def forward(self, seqs, seq_lens):\n",
    "        batch_size = seqs.size(0)\n",
    "        target_seq_len = seqs.size(1) + 1\n",
    "        embeded = self.embed(seqs)\n",
    "        \n",
    "        padded = pack_padded_sequence(embeded, seq_lens, batch_first=True)\n",
    "        h0 = Variable(torch.zeros([2, batch_size, vector_dim])).cuda()\n",
    "        _, h = self.encode(padded, h0, )\n",
    "        h = torch.cat([h[0,...], h[1,...]], dim=1).unsqueeze(dim=0)\n",
    "        \n",
    "        ys = []\n",
    "        # first input to decoder is EOS, which is 1 in index\n",
    "        y = Variable(torch.ones([batch_size, 1])).long().cuda()\n",
    "        y = self.embed(y)\n",
    "        for i in range(target_seq_len):\n",
    "            y, h = self.decode(y, h)\n",
    "            ys.append(y)\n",
    "            y = self.next_decoder_input(y)\n",
    "\n",
    "            \n",
    "        out = torch.cat(ys, dim=1)\n",
    "        \n",
    "        logits = self.project(out.view([-1, vector_dim*2]))\n",
    "        return logits.view([batch_size, target_seq_len, vocab_size])\n",
    "    \n",
    "    def next_decoder_input(self, decoder_output):\n",
    "        logits = self.project(decoder_output)\n",
    "        _, label = torch.max(logits, dim=2)\n",
    "        embed = self.embed(label)\n",
    "        return embed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((16, 5), (16, 6), torch.Size([16, 6, 10]))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## test model\n",
    "seqs, targets, lens = zip(*[toy_ds[i] for i in range(16)])\n",
    "seqs, targets, lens = sort_seqs_by_len(seqs, targets, lens=lens)\n",
    "\n",
    "x = Variable(torch.from_numpy(seqs)).cuda()\n",
    "m = BidiSeq2Seq().cuda()\n",
    "y = m(x, lens)\n",
    "seqs.shape, targets.shape, y.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## training\n",
    "\n",
    "## make it a little more challenging by using a longer max_seq\n",
    "toy_ds = ReverseSeqData(n_data=50000, max_seq=5)\n",
    "\n",
    "batch_size = 128\n",
    "n_batches = len(toy_ds) // batch_size\n",
    "n_epochs = 50\n",
    "\n",
    "objective = nn.CrossEntropyLoss()\n",
    "\n",
    "model = BidiSeq2Seq().cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0 2.318190097808838\n",
      "5 0 0.27784043550491333\n",
      "10 0 0.10735460370779037\n",
      "15 0 0.06630130857229233\n",
      "20 0 0.023480141535401344\n",
      "25 0 0.025113224983215332\n",
      "30 0 0.006958003621548414\n",
      "35 0 0.0022069981787353754\n",
      "40 0 0.0009632504661567509\n",
      "45 0 0.0015089333755895495\n",
      "CPU times: user 18min 54s, sys: 2min 49s, total: 21min 43s\n",
      "Wall time: 21min 48s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "model.train()\n",
    "\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "index = np.arange(0, len(toy_ds))\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    \n",
    "    np.random.shuffle(index)\n",
    "    for b, bi in enumerate(np.array_split(index, n_batches)):\n",
    "        seqs, targets, lens = zip(*[toy_ds[i] for i in bi])\n",
    "        seqs, targets, lens = sort_seqs_by_len(seqs, targets, lens=lens)\n",
    "        \n",
    "        x = Variable(torch.from_numpy(seqs)).cuda()\n",
    "        y = Variable(torch.from_numpy(targets)).cuda()\n",
    "        logits = model(x, lens)\n",
    "        \n",
    "        loss = objective(logits.view([-1, vocab_size]), y.view([-1]))\n",
    "        model.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if epoch % 5 == 0 and b % n_batches == 0:\n",
    "            print(epoch, b, loss.data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "## evaluation\n",
    "model.eval()\n",
    "seqs, targets, lens = zip(*[toy_ds[i] for i in range(20)])\n",
    "seqs, targets, lens = sort_seqs_by_len(seqs, targets, lens=lens)\n",
    "\n",
    "x = Variable(torch.from_numpy(seqs)).cuda()\n",
    "y = model(x, lens)\n",
    "_, label = torch.max(y, dim=-1)\n",
    "print(\"accuracy:\", np.mean(label.data.cpu().numpy() == targets))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Appendix - Understand behavior of RNN (e.g. GRU) on padded sequence\n",
    "- its output is from the last layer (if multiple layers involved) for each time step, and if the input is a padded sequence, the output will also be padded (meaning, zeros after certrain step)\n",
    "- its hidden state is essentially the value of the last effective step of outputs along the time step, considering the padding of the sequence. It has similiar interface as tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 3, 2])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = Variable(torch.LongTensor([\n",
    "    [1, 2, 3], \n",
    "    [1, 2, 0], \n",
    "    [1, 0, 0]])).cuda()\n",
    "embed = nn.Embedding(num_embeddings=4, embedding_dim=2, padding_idx=0).cuda()\n",
    "x = embed(x)\n",
    "x.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[ 0.56909215, -0.58901101],\n",
       "        [ 1.51466238, -1.08357263],\n",
       "        [ 1.0598985 , -1.89443576]],\n",
       "\n",
       "       [[ 0.56909215, -0.58901101],\n",
       "        [ 1.51466238, -1.08357263],\n",
       "        [ 0.        ,  0.        ]],\n",
       "\n",
       "       [[ 0.56909215, -0.58901101],\n",
       "        [ 0.        ,  0.        ],\n",
       "        [ 0.        ,  0.        ]]], dtype=float32)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.data.cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([3, 3, 2]), torch.Size([2, 3, 1]))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "padx = pack_padded_sequence(x, [3, 2, 1])\n",
    "\n",
    "rnn = nn.GRU(input_size=2, hidden_size=1, batch_first=True, bidirectional=True).cuda()\n",
    "h0 = Variable(torch.zeros(2, 3, 1)).cuda()\n",
    "y, h = rnn(padx, h0)\n",
    "y, lens = pad_packed_sequence(y, batch_first=True)\n",
    "y.size(), h.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       "(0 ,.,.) = \n",
       " -0.1994 -0.9486\n",
       " -0.3485 -0.8866\n",
       " -0.4583 -0.7033\n",
       "\n",
       "(1 ,.,.) = \n",
       " -0.2398 -0.9419\n",
       " -0.4255 -0.7900\n",
       "  0.0000  0.0000\n",
       "\n",
       "(2 ,.,.) = \n",
       " -0.1587 -0.6753\n",
       "  0.0000  0.0000\n",
       "  0.0000  0.0000\n",
       "[torch.cuda.FloatTensor of size 3x3x2 (GPU 0)]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       "(0 ,.,.) = \n",
       " -0.4583\n",
       " -0.4255\n",
       " -0.1587\n",
       "\n",
       "(1 ,.,.) = \n",
       " -0.9486\n",
       " -0.9419\n",
       " -0.6753\n",
       "[torch.cuda.FloatTensor of size 2x3x1 (GPU 0)]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
