{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Varational autoencoder by RNN for sudoku sequence generation\n",
    "\n",
    "## dataset \n",
    "- [10,000 solved sudoku](http://www.printable-sudoku-puzzles.com/wfiles/)\n",
    "- [bigger dataset: 1M sudoku and solutions](https://www.kaggle.com/bryanpark/sudoku)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load sudoku training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from glob import glob\n",
    "\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "symbol2index = dict(zip('123456789', range(9)))\n",
    "index2symbol = dict(zip(range(9), '123456789'))\n",
    "\n",
    "def puzzle2tensor(puzzle_batch):\n",
    "    batch_size = len(puzzle_batch)\n",
    "    seq_len = len(puzzle_batch[0])\n",
    "    t = torch.zeros([batch_size, seq_len, 9])\n",
    "    for r in range(batch_size):\n",
    "        for c in range(seq_len):\n",
    "            s = symbol2index[puzzle_batch[r][c]]\n",
    "            t[r, c, s] = 1\n",
    "    return t\n",
    "\n",
    "def puzzle2target(puzzle_batch):\n",
    "    batch_size = len(puzzle_batch)\n",
    "    seq_len = len(puzzle_batch[0])\n",
    "    t = torch.LongTensor(batch_size, seq_len).zero_()\n",
    "    for r in range(batch_size):\n",
    "        for c in range(seq_len):\n",
    "            s = symbol2index[puzzle_batch[r][c]]\n",
    "            t[r, c] = s\n",
    "    return t\n",
    "\n",
    "def tensor2puzzle(tensors):\n",
    "    \"\"\"tensors.size() == [batch_size, seq, 9]\n",
    "    \"\"\"\n",
    "    _, p = tensors.max(dim=2)\n",
    "    p = p.squeeze().numpy()\n",
    "    puzzles = []\n",
    "    for r in p:\n",
    "        puzzles.append(''.join([index2symbol.get(s) for s in r]))\n",
    "    return puzzles\n",
    "\n",
    "def output2puzzle(y):\n",
    "    \"\"\"y.size() == [batch_size, 9]\n",
    "    \"\"\"\n",
    "    _, labels = y.max(dim=1)\n",
    "    return labels.numpy().squeeze() + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## build a sudoku env following openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Sudoku(object):\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.state = None\n",
    "        \n",
    "    def reset(self):\n",
    "        \"\"\"return observation\"\"\"\n",
    "        self.state = []\n",
    "        obs = ''.join(self.state)\n",
    "        return obs\n",
    "    \n",
    "    def step(self, action):\n",
    "        \"\"\"action: '123456789'\n",
    "        return [obs, reward, done, info]\"\"\"\n",
    "        self.state.append(action)\n",
    "        obs = ''.join(self.state)\n",
    "        reward = 1\n",
    "        valid, info = self.is_valid(self.state)\n",
    "        done = not valid\n",
    "        return [obs, reward, done, info]\n",
    "    \n",
    "    def is_valid(self, puzzle):\n",
    "        \"\"\"puzzle: an array of digits.\n",
    "        Check if a partial puzzle is valid\n",
    "        \"\"\"\n",
    "        nrows = int(np.ceil(len(puzzle)/9))\n",
    "        for r in range(0, nrows):\n",
    "            row = puzzle[r*9:(r+1)*9]\n",
    "            if len(row) != len(set(row)):\n",
    "                return False, \"row rule\"\n",
    "        for c in range(0, 9):\n",
    "            col = puzzle[c:c+9*9:9]\n",
    "            if len(col) != len(set(col)):\n",
    "                return False, \"col rule\"\n",
    "        strides = [list(range(0, 3)), list(range(3, 6)), list(range(6, 9))]\n",
    "        squares = [(r, c) for r in strides for c in strides]\n",
    "        for sqr, sqc in squares:\n",
    "            sqi = [r*9+c for r in sqr for c in sqc]\n",
    "            square = [puzzle[i] for i in sqi if i < len(puzzle)]\n",
    "            if len(square) != len(set(square)):\n",
    "                return False, \"sqr rule\"\n",
    "        return True, \"valid\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['1', 1, False, 'valid']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env = Sudoku()\n",
    "print(env.reset())\n",
    "env.step('1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(False, 'col rule')\n",
      "(True, 'valid')\n"
     ]
    }
   ],
   "source": [
    "puzzle = list(\"123456789987654321\")\n",
    "print(env.is_valid(puzzle))\n",
    "puzzle = list(\"123456\")\n",
    "print(env.is_valid(puzzle[:-5]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## baseline - random generator policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def random_policy():\n",
    "    return np.random.choice( list(\"123456789\"), 1)[0] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "all_rewards = []\n",
    "\n",
    "env = Sudoku()\n",
    "\n",
    "for game in range(5000):\n",
    "    obs = env.reset()\n",
    "    game_reward = 0\n",
    "    for _ in range(81):\n",
    "        action = random_policy()\n",
    "        obs, reward, done, info = env.step(action)\n",
    "        game_reward += reward\n",
    "        if done: break\n",
    "    all_rewards.append(game_reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYEAAAD9CAYAAABazssqAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nsQSlMQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7ff03437ac88>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "_ = plt.hist(all_rewards)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## rnn model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class SudokuPolicy(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SudokuPolicy, self).__init__()\n",
    "        self.rnn = nn.GRU(input_size=9, hidden_size=128,\n",
    "                          num_layers=1, bidirectional=False,\n",
    "                          batch_first=True)\n",
    "        self.fc1 = nn.Linear(128, 64)\n",
    "        self.elu = nn.ELU()\n",
    "        self.d = nn.Dropout(0.5)\n",
    "        self.fc2 = nn.Linear(64, 64)\n",
    "        self.fc3 = nn.Linear(64, 9)\n",
    "        self.softmax = nn.Softmax()\n",
    "    def forward(self, obs):\n",
    "        batch_size, seq_len, input_size = obs.size()\n",
    "        h0 = Variable(torch.zeros(1, batch_size, 128)).cuda()\n",
    "        out, _ = self.rnn(obs, h0)\n",
    "        out = out[:, -1, :]\n",
    "        out = self.elu(self.fc1(out))\n",
    "        out = self.d(out)\n",
    "        out = self.elu(self.fc2(out))\n",
    "        out = self.d(out)\n",
    "        logits = self.fc3(out)\n",
    "        # probs and labels for action calculation\n",
    "        probs = self.softmax(logits)\n",
    "        self.label = torch.multinomial(probs, 1)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable containing:\n",
      "-0.0799 -0.1496 -0.1107  0.0558  0.0498  0.0982  0.1732  0.0830 -0.1403\n",
      "[torch.cuda.FloatTensor of size 1x9 (GPU 0)]\n",
      "\n",
      "9\n"
     ]
    }
   ],
   "source": [
    "actions = \"123456789\"\n",
    "\n",
    "m = SudokuPolicy().cuda()\n",
    "obs = Variable(puzzle2tensor([\"12\"])).cuda()\n",
    "print( m(obs) )\n",
    "print( actions[m.label.data.cpu().numpy()[0][0]] )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## credit assignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def discount_rewards(rewards, discount_rate):\n",
    "    \"\"\"rewards: rewards of all steps for a single game, a list\n",
    "    \"\"\"\n",
    "    discounted_rewards = np.empty(len(rewards),dtype=np.float32)\n",
    "    accum_reward = 0\n",
    "    for i in reversed(range(len(rewards))):\n",
    "        accum_reward = rewards[i] + accum_reward * discount_rate\n",
    "        discounted_rewards[i] = accum_reward\n",
    "    return discounted_rewards\n",
    "\n",
    "def discount_and_normalize_rewards(all_rewards, discount_rate):\n",
    "    \"\"\"\n",
    "    all_rewards: rewards of all games for all steps, a list of list\n",
    "    \"\"\"\n",
    "    discounted_all_rewards = [discount_rewards(rewards, discount_rate)\n",
    "                             for rewards in all_rewards]\n",
    "    flatten_rewards = [reward for rewards in discounted_all_rewards\n",
    "                          for reward in rewards]\n",
    "    reward_mean = np.mean(flatten_rewards)\n",
    "    reward_std = np.std(flatten_rewards)\n",
    "    normalized_rewards = [(rewards - reward_mean) / reward_std\n",
    "                         for rewards in discounted_all_rewards]\n",
    "    return normalized_rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-22., -40., -50.], dtype=float32)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "discount_rewards([10, 0, -50], discount_rate=.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([-0.28435072, -0.86597717, -1.18910301], dtype=float32),\n",
       " array([ 1.2666533 ,  1.07277775], dtype=float32)]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "discount_and_normalize_rewards([[10, 0, -50], [10, 20]],\n",
    "                               discount_rate=0.8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## training by policy gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SudokuPolicy (\n",
       "  (rnn): GRU(9, 128, batch_first=True)\n",
       "  (fc1): Linear (128 -> 64)\n",
       "  (elu): ELU (alpha=1.0)\n",
       "  (d): Dropout (p = 0.5)\n",
       "  (fc2): Linear (64 -> 64)\n",
       "  (fc3): Linear (64 -> 9)\n",
       "  (softmax): Softmax ()\n",
       ")"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "objective = nn.CrossEntropyLoss()\n",
    "\n",
    "model = SudokuPolicy().cuda()\n",
    "model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 3.71 8\n",
      "25 4.58 8\n",
      "50 6.0 11\n",
      "75 6.48 13\n",
      "100 7.11 14\n",
      "125 7.97 16\n",
      "150 9.05 16\n",
      "175 8.59 13\n",
      "200 9.09 16\n",
      "225 10.12 16\n",
      "250 8.21 18\n",
      "275 9.47 18\n",
      "300 9.69 18\n",
      "325 10.03 18\n",
      "350 10.87 18\n",
      "375 10.78 18\n",
      "400 11.22 18\n",
      "425 10.29 18\n",
      "450 11.02 18\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "n_epochs = 600\n",
    "n_max_steps = 81\n",
    "n_games_per_update = 100\n",
    "discount_rate = .95\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-2)\n",
    "actions = \"123456789\"\n",
    "\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    \n",
    "    all_losses = []\n",
    "    all_rewards = []\n",
    "    \n",
    "    for game in range(n_games_per_update):\n",
    "        \n",
    "        game_losses = []\n",
    "        game_rewards = []\n",
    "        \n",
    "        obs = env.reset()\n",
    "        first_action = random_policy() # random first action\n",
    "        obs, reward, done, info = env.step(first_action)\n",
    "        \n",
    "        for step in range(n_max_steps):\n",
    "            # train on one observation\n",
    "            obs_var = Variable(puzzle2tensor([obs])).cuda()\n",
    "            model.zero_grad()\n",
    "            logits = model(obs_var)\n",
    "            label = model.label.data.cpu().numpy()[0][0]\n",
    "            action = actions[label]\n",
    "            obs, reward, done, info = env.step(action)\n",
    "\n",
    "            y = Variable(torch.LongTensor([int(label)])).cuda()\n",
    "            loss = objective(logits, y)\n",
    "            game_losses.append(loss)\n",
    "            game_rewards.append(reward)\n",
    "            if done:\n",
    "                break\n",
    "                \n",
    "        all_losses.append(game_losses)\n",
    "        all_rewards.append(game_rewards)\n",
    "    \n",
    "    ## normalize and discount rewards\n",
    "    normalized_rewards = discount_and_normalize_rewards(all_rewards, discount_rate=discount_rate)\n",
    "    ## aggregate losses\n",
    "    weighted_loss = np.mean([all_losses[game][step] * float(normalized_rewards[game][step])\n",
    "                        for game in range(len(normalized_rewards))\n",
    "                        for step in range(len(normalized_rewards[game]))])\n",
    "    model.zero_grad()\n",
    "    weighted_loss.backward()\n",
    "    ## optimize based on aggregated gradient\n",
    "    optimizer.step()\n",
    "    if epoch % 25 == 0:\n",
    "        rewards = [np.sum(reward) for reward in all_rewards]\n",
    "        mean_reward = np.mean(rewards)\n",
    "        max_reward = np.max(rewards)\n",
    "        print(epoch, mean_reward, max_reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
