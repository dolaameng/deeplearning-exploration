{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Policy Gradient for Cart Pole Reinforcement Learning\n",
    "\n",
    "- From Chapter 16 in \"Handson Machine Learning with Scikit-Learn and Tensorflow\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import gym\n",
    "\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.autograd import Variable\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cart Pole from openai gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-06-15 21:04:09,709] Making new env: CartPole-v1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([-0.01322629, -0.01979786, -0.0343556 ,  0.03389911])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env = gym.make(\"CartPole-v1\")\n",
    "env.reset()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "plt.imshow(env.render(mode=\"rgb_array\", close=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def basic_policy(obs):\n",
    "    # obs = (position, speed, angel, angel_speed)\n",
    "    angel = obs[2]\n",
    "    # action 0 to left 1 to right\n",
    "    return 0 if angel < 0 else 1\n",
    "\n",
    "## test basic policy\n",
    "all_rewards = []\n",
    "for igame in range(500):\n",
    "    obs = env.reset()\n",
    "    game_rewards = 0\n",
    "    for step in range(1000):\n",
    "        action = basic_policy(obs)\n",
    "        obs, reward, done, info = env.step(action)\n",
    "        if done: break\n",
    "        game_rewards += reward\n",
    "    all_rewards.append(game_rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAD8CAYAAAB5Pm/hAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nYoqed1S0kgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f1e89a60be0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "_ = plt.hist(all_rewards)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train a MLP based on Policy Gradient\n",
    "- training on one observation each time?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class CartPoleModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CartPoleModel, self).__init__()\n",
    "        self.fc1 = nn.Linear(4, 4)\n",
    "        self.elu = nn.ELU()\n",
    "        self.fc2 = nn.Linear(4, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "    def forward(self, x):\n",
    "        out = self.elu(self.fc1(x))\n",
    "        logits = self.fc2(out)\n",
    "        probs = self.sigmoid(logits)\n",
    "        self.label = torch.multinomial(torch.cat([probs, 1-probs], dim=1), 1)\n",
    "\n",
    "        return probs\n",
    "    def output(self):\n",
    "        return self.label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Variable containing:\n",
       "  0.5090\n",
       " [torch.cuda.FloatTensor of size 1x1 (GPU 0)], Variable containing:\n",
       "  1\n",
       " [torch.cuda.LongTensor of size 1x1 (GPU 0)])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m = CartPoleModel().cuda()\n",
    "x = Variable(torch.from_numpy(obs.astype(np.float32))[None, ...]).cuda()\n",
    "m(x), m.output()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utility for Credit Assignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def discount_rewards(rewards, discount_rate):\n",
    "    \"\"\"rewards: rewards of all steps for a single game, a list\n",
    "    \"\"\"\n",
    "    discounted_rewards = np.empty(len(rewards),dtype=np.float32)\n",
    "    accum_reward = 0\n",
    "    for i in reversed(range(len(rewards))):\n",
    "        accum_reward = rewards[i] + accum_reward * discount_rate\n",
    "        discounted_rewards[i] = accum_reward\n",
    "    return discounted_rewards\n",
    "\n",
    "def discount_and_normalize_rewards(all_rewards, discount_rate):\n",
    "    \"\"\"\n",
    "    all_rewards: rewards of all games for all steps, a list of list\n",
    "    \"\"\"\n",
    "    discounted_all_rewards = [discount_rewards(rewards, discount_rate)\n",
    "                             for rewards in all_rewards]\n",
    "    flatten_rewards = [reward for rewards in discounted_all_rewards\n",
    "                          for reward in rewards]\n",
    "    reward_mean = np.mean(flatten_rewards)\n",
    "    reward_std = np.std(flatten_rewards)\n",
    "    normalized_rewards = [(rewards - reward_mean) / reward_std\n",
    "                         for rewards in discounted_all_rewards]\n",
    "    return normalized_rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-22., -40., -50.], dtype=float32)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "discount_rewards([10, 0, -50], discount_rate=.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([-0.28435072, -0.86597717, -1.18910301], dtype=float32),\n",
       " array([ 1.2666533 ,  1.07277775], dtype=float32)]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "discount_and_normalize_rewards([[10, 0, -50], [10, 20]],\n",
    "                               discount_rate=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "float"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = discount_and_normalize_rewards([[10, 0, -50], [10, 20]],\n",
    "                               discount_rate=0.8)\n",
    "type(float(a[0][0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training - method 1\n",
    "- three layers loop\n",
    "    - for each epoch (update parameters)\n",
    "    - for each game\n",
    "    - for each step in the game\n",
    "- the collection of rewards will be a list of list\n",
    "    - for each game\n",
    "    - for each step\n",
    "- the collection of gradients of be a list of list of tensor\n",
    "    - for each game, each step\n",
    "    - a gradient tensor\n",
    "- at each iteration, the gradient will be weighted by normalized-discounted rewards, and aggregated as mean to update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 18.7\n",
      "25 20.0\n",
      "50 16.7\n",
      "75 28.6\n",
      "100 21.3\n",
      "125 28.4\n",
      "150 19.6\n",
      "175 20.9\n",
      "200 15.6\n",
      "225 20.2\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 250\n",
    "n_max_steps = 1000\n",
    "n_games_per_update = 10\n",
    "discount_rate = .95\n",
    "\n",
    "objective = nn.BCELoss()\n",
    "\n",
    "model = CartPoleModel().cuda()\n",
    "model.train()\n",
    "\n",
    "params = list(model.parameters())\n",
    "\n",
    "optimizer = optim.Adam(params, lr=1e-2)\n",
    "\n",
    "\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    \n",
    "    all_gradients = []\n",
    "    all_rewards = []\n",
    "    \n",
    "    for game in range(n_games_per_update):\n",
    "        \n",
    "        game_gradients = []\n",
    "        game_rewards = []\n",
    "        obs = env.reset()\n",
    "        \n",
    "        for step in range(n_max_steps):\n",
    "            # train on one observation\n",
    "            obs_var = Variable(torch.from_numpy(obs.astype(np.float32))[None, ...]).cuda()\n",
    "            model.zero_grad()\n",
    "            prob = model(obs_var)\n",
    "            action = model.output().data[0][0]#.cpu().numpy()\n",
    "#             action = prob.multinomial().data[0][0] # this doesnt work as expected! prob needs to be a vector!\n",
    "            obs, reward, done, info = env.step(action)\n",
    "            # action: 0 left 1 right, y: prob of left\n",
    "            y = Variable(torch.FloatTensor([1. - action]))[None, ...].cuda()\n",
    "            loss = objective(prob, y)\n",
    "            loss.backward()\n",
    "            gradient = [p.grad for p in params]\n",
    "            game_gradients.append(gradient)\n",
    "            game_rewards.append(reward)\n",
    "            if done:\n",
    "                break\n",
    "                \n",
    "        all_gradients.append(game_gradients)\n",
    "        all_rewards.append(game_rewards)\n",
    "    \n",
    "    ## normalize and discount rewards\n",
    "    normalized_rewards = discount_and_normalize_rewards(all_rewards, discount_rate=discount_rate)\n",
    "    ## aggregate gradients\n",
    "    for param in range(len(params)):\n",
    "        mean_gradient = np.mean([ float(normalized_rewards[game][step])*step_gradients[param].data \n",
    "                                for game, game_gradients in enumerate(all_gradients)\n",
    "                                for step, step_gradients in enumerate(game_gradients)])\n",
    "#         print(mean_gradient)\n",
    "        params[param].grad.data.set_(mean_gradient)\n",
    "    ## optimize based on aggregated gradient\n",
    "    optimizer.step()\n",
    "    if epoch % 25 == 0:\n",
    "        mean_reward = np.mean([np.sum(reward) for reward in all_rewards])\n",
    "        print(epoch, mean_reward)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "*** It didn't work very well... maybe the naive way of updating the gradient didn't work?***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training - method 2\n",
    "\n",
    "Instead of computing weighted mean of gradient, we can also compute the weighted mean of loss function, and do the normal backpropagation\n",
    "\n",
    "*** Lesson Learned ***: Try to avoid manipulate the gradient directly in pytorch, if possible, modify the loss function or use other ways."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 17.3\n",
      "25 30.3\n",
      "50 52.0\n",
      "75 126.4\n",
      "100 240.0\n",
      "125 302.1\n",
      "150 447.4\n",
      "175 405.8\n",
      "CPU times: user 15min 24s, sys: 1.74 s, total: 15min 26s\n",
      "Wall time: 15min 25s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "n_epochs = 200\n",
    "n_max_steps = 1000\n",
    "n_games_per_update = 10\n",
    "discount_rate = .95\n",
    "\n",
    "objective = nn.BCELoss()\n",
    "\n",
    "model = CartPoleModel().cuda()\n",
    "model.train()\n",
    "\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-2)\n",
    "\n",
    "\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    \n",
    "    all_losses = []\n",
    "    all_rewards = []\n",
    "    \n",
    "    for game in range(n_games_per_update):\n",
    "        \n",
    "        game_losses = []\n",
    "        game_rewards = []\n",
    "        \n",
    "        obs = env.reset()\n",
    "        \n",
    "        for step in range(n_max_steps):\n",
    "            # train on one observation\n",
    "            obs_var = Variable(torch.from_numpy(obs.astype(np.float32))[None, ...]).cuda()\n",
    "            model.zero_grad()\n",
    "            prob = model(obs_var)\n",
    "            action = model.output().data[0][0]#.cpu().numpy()\n",
    "#             action = prob.multinomial().data[0][0] # this doesn't work as expected, prob should be a softmax vector\n",
    "            obs, reward, done, info = env.step(action)\n",
    "            # action: 0 left 1 right, y: prob of left\n",
    "            y = Variable(torch.FloatTensor([1. - action]))[None, ...].cuda()\n",
    "            loss = objective(prob, y)\n",
    "            game_losses.append(loss)\n",
    "            game_rewards.append(reward)\n",
    "            if done:\n",
    "                break\n",
    "                \n",
    "        all_losses.append(game_losses)\n",
    "        all_rewards.append(game_rewards)\n",
    "    \n",
    "    ## normalize and discount rewards\n",
    "    normalized_rewards = discount_and_normalize_rewards(all_rewards, discount_rate=discount_rate)\n",
    "    ## aggregate losses\n",
    "    weighted_loss = np.mean([all_losses[game][step] * float(normalized_rewards[game][step])\n",
    "                        for game in range(len(normalized_rewards))\n",
    "                        for step in range(len(normalized_rewards[game]))])\n",
    "    model.zero_grad()\n",
    "    weighted_loss.backward()\n",
    "    ## optimize based on aggregated gradient\n",
    "    optimizer.step()\n",
    "    if epoch % 25 == 0:\n",
    "        mean_reward = np.mean([np.sum(reward) for reward in all_rewards])\n",
    "        print(epoch, mean_reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAD8CAYAAAB5Pm/hAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nSY0z6CWpcQa9JDXOoJekxhn0ktS4/wNBny4FqwsC7wAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f1e7c1360f0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## test the model\n",
    "\n",
    "from tqdm import tqdm_notebook\n",
    "def model_policy(model, obs):\n",
    "    obs_var = Variable(torch.from_numpy(obs.astype(np.float32))[None, ...]).cuda()\n",
    "    prob = model(obs_var)\n",
    "    action = model.output().data[0][0]\n",
    "    return action\n",
    "\n",
    "## test basic policy\n",
    "all_rewards = []\n",
    "for igame in tqdm_notebook(range(500)):\n",
    "    obs = env.reset()\n",
    "    game_rewards = 0\n",
    "    for step in range(1000):\n",
    "        action = model_policy(model, obs)\n",
    "        obs, reward, done, info = env.step(action)\n",
    "        if done: break\n",
    "        game_rewards += reward\n",
    "    all_rewards.append(game_rewards)\n",
    "    \n",
    "_ = plt.hist(all_rewards)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train - Method 3\n",
    "\n",
    "Another method can be found in the [official tutorial](https://github.com/pytorch/examples/blob/master/reinforcement_learning/reinforce.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  },
  "widgets": {
   "state": {
    "d983e3b7b98746fe99f7d77cc7527067": {
     "views": [
      {
       "cell_index": 22
      }
     ]
    }
   },
   "version": "1.2.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
